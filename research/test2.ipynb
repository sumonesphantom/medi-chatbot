{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e0ab48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/venkat/anaconda3/envs/medibot/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 637 documents.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import List, Optional\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from langchain_core.language_models.llms import LLM\n",
    "from langchain.schema import Document\n",
    "from langchain.document_loaders import DirectoryLoader, PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "# -------------------------\n",
    "# Load PDF files\n",
    "# -------------------------\n",
    "def load_pdf_files(data_folder: str):\n",
    "    loader = DirectoryLoader(\n",
    "        data_folder,\n",
    "        glob=\"*.pdf\",\n",
    "        loader_cls=PyPDFLoader\n",
    "    )\n",
    "    documents = loader.load()\n",
    "    return documents\n",
    "\n",
    "extracted_data = load_pdf_files(\"../data\")\n",
    "print(f\"Loaded {len(extracted_data)} documents.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fd0826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks: 5859\n",
      "Vector length: 384\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# -------------------------\n",
    "# Filter minimal docs\n",
    "# -------------------------\n",
    "def filter_to_minimal_docs(docs: List[Document]) -> List[Document]:\n",
    "    minimal_docs: List[Document] = []\n",
    "    for doc in docs:\n",
    "        src = doc.metadata.get(\"source\")\n",
    "        minimal_docs.append(Document(page_content=doc.page_content, metadata={\"source\": src}))\n",
    "    return minimal_docs\n",
    "\n",
    "minimal_docs = filter_to_minimal_docs(extracted_data)\n",
    "\n",
    "# -------------------------\n",
    "# Split documents into chunks\n",
    "# -------------------------\n",
    "def text_split(minimal_docs: List[Document]):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=500,\n",
    "        chunk_overlap=20,\n",
    "    )\n",
    "    return text_splitter.split_documents(minimal_docs)\n",
    "\n",
    "texts_chunk = text_split(minimal_docs)\n",
    "print(f\"Number of chunks: {len(texts_chunk)}\")\n",
    "\n",
    "# -------------------------\n",
    "# Load HuggingFace embeddings\n",
    "# -------------------------\n",
    "# embedding_model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "# embedding = SentenceTransformer(embedding_model_name)\n",
    "\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embedding = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")\n",
    "\n",
    "\n",
    "# Test embedding\n",
    "vector = embedding.embed_query(\"Hello world\")\n",
    "print(\"Vector length:\", len(vector))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9916078",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -------------------------\n",
    "#  Setup Pinecone\n",
    "# -------------------------\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "os.environ[\"PINECONE_API_KEY\"] = PINECONE_API_KEY\n",
    "\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "index_name = \"medical-chatbot\"\n",
    "\n",
    "if not pc.has_index(index_name):\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=384,  # Must match embedding dimension\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")\n",
    "    )\n",
    "\n",
    "index = pc.Index(index_name)\n",
    "\n",
    "# -------------------------\n",
    "# 6️⃣ Create vector store\n",
    "# -------------------------\n",
    "vectorstore = PineconeVectorStore(\n",
    "    index=index,\n",
    "    embedding=embedding\n",
    ")\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\":3})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff26f777",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/venkat/anaconda3/envs/medibot/lib/python3.10/site-packages/pydantic/_internal/_generate_schema.py:648: ArbitraryTypeWarning: <built-in function any> is not a Python type (it may be an instance of an object), Pydantic will allow any object with no validation since we cannot even enforce that the input is an instance of the given type. To get rid of this error wrap the type with `pydantic.SkipValidation`.\n",
      "  warnings.warn(\n",
      "Some parameters are on the meta device because they were offloaded to the cpu and disk.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# -------------------------\n",
    "# Load Local Hugging Face LLM (TinyLlama)\n",
    "# -------------------------\n",
    "class LocalHFLLM(LLM):\n",
    "    pipeline: any\n",
    "\n",
    "    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
    "        outputs = self.pipeline(\n",
    "            prompt,\n",
    "            max_new_tokens=256,\n",
    "            temperature=0.2,\n",
    "            do_sample=True\n",
    "        )\n",
    "        return outputs[0][\"generated_text\"][len(prompt):]\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"local_hf\"\n",
    "\n",
    "model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=\"auto\"\n",
    ")\n",
    "\n",
    "hf_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "chatModel = LocalHFLLM(pipeline=hf_pipeline)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3f9ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -------------------------\n",
    "# Create RAG chain\n",
    "# -------------------------\n",
    "system_prompt = (\n",
    "    \"You are a medical assistant. Use ONLY the provided context to answer. \"\n",
    "    \"If the answer is not in the context, say you do not know. \"\n",
    "    \"Use at most three concise sentences.\\n\\n{context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "qa_chain = create_stuff_documents_chain(\n",
    "    llm=chatModel,\n",
    "    prompt=prompt\n",
    ")\n",
    "\n",
    "rag_chain = create_retrieval_chain(\n",
    "    retriever=retriever,\n",
    "    combine_docs_chain=qa_chain\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "35465ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: \n",
      "Acromegaly is a disorder in which the abnormal release of a\n",
      "particular chemical from the pituitary gland in the brain causes\n",
      "increased growth in bone and soft tissues, as well as a variety\n",
      "of other disturbances throughout the body. This chemical released\n",
      "from the pituitary gland\n",
      "\n",
      "Whitehouse Station, NJ: Merck Research Laboratories, 1997.\n",
      "Larsen, D. E., ed. Mayo Clinic Family Health Book.New York:\n",
      "William Morrow and Co., Inc., 1996.\n",
      "John T. Lohr, PhD\n",
      "Acromegaly and gigantism\n",
      "Definition\n",
      "Acromegaly is a disorder in which the abnormal release of a\n",
      "particular chemical from the pituitary gland in the brain causes\n",
      "increased growth in bone and soft tissues, as well as a variety\n",
      "of other disturbances throughout the body. This chemical released\n",
      "from the pituitary gland\n",
      "\n",
      "mone Excess: Acromegaly and Gigantism.” In Harrison’s\n",
      "Principles of Internal Medicine\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# -------------------------\n",
    "# 9️⃣ Run query\n",
    "# -------------------------\n",
    "query = \"What is Acromegaly and gigantism?\"\n",
    "response = rag_chain.invoke({\"input\": query})\n",
    "print(\"Answer:\", response[\"answer\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medibot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
