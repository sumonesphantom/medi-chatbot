{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc841a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbd3abd9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain.document_loaders'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocument_loaders\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PyPDFLoader, DirectoryLoader\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext_splitter\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RecursiveCharacterTextSplitter\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'langchain.document_loaders'"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1287ed6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DirectoryLoader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m     documents \u001b[38;5;241m=\u001b[39m loader\u001b[38;5;241m.\u001b[39mload()\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m documents\n\u001b[0;32m---> 12\u001b[0m extracted_data \u001b[38;5;241m=\u001b[39m \u001b[43mload_pdf_files\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(extracted_data))\n\u001b[1;32m     14\u001b[0m extracted_data\n",
      "Cell \u001b[0;32mIn[3], line 3\u001b[0m, in \u001b[0;36mload_pdf_files\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_pdf_files\u001b[39m(data):\n\u001b[0;32m----> 3\u001b[0m     loader \u001b[38;5;241m=\u001b[39m \u001b[43mDirectoryLoader\u001b[49m(\n\u001b[1;32m      4\u001b[0m         data,\n\u001b[1;32m      5\u001b[0m         glob\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*.pdf\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      6\u001b[0m         loader_cls\u001b[38;5;241m=\u001b[39mPyPDFLoader\n\u001b[1;32m      7\u001b[0m     )\n\u001b[1;32m      9\u001b[0m     documents \u001b[38;5;241m=\u001b[39m loader\u001b[38;5;241m.\u001b[39mload()\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m documents\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DirectoryLoader' is not defined"
     ]
    }
   ],
   "source": [
    "# Extract text from PDF files\n",
    "def load_pdf_files(data):\n",
    "    loader = DirectoryLoader(\n",
    "        data,\n",
    "        glob=\"*.pdf\",\n",
    "        loader_cls=PyPDFLoader\n",
    "    )\n",
    "\n",
    "    documents = loader.load()\n",
    "    return documents\n",
    "\n",
    "extracted_data = load_pdf_files(\"data\")\n",
    "print(len(extracted_data))\n",
    "extracted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ab351e3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain.schema'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m List\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mschema\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Document\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfilter_to_minimal_docs\u001b[39m(docs: List[Document]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Document]:\n\u001b[1;32m      5\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124;03m    Given a list of Document objects, return a new list of Document objects\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124;03m    containing only 'source' in metadata and the original page_content.\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'langchain.schema'"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "from langchain.schema import Document\n",
    "\n",
    "def filter_to_minimal_docs(docs: List[Document]) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Given a list of Document objects, return a new list of Document objects\n",
    "    containing only 'source' in metadata and the original page_content.\n",
    "    \"\"\"\n",
    "    minimal_docs: List[Document] = []\n",
    "    for doc in docs:\n",
    "        src = doc.metadata.get(\"source\")\n",
    "        minimal_docs.append(\n",
    "            Document(\n",
    "                page_content=doc.page_content,\n",
    "                metadata={\"source\": src}\n",
    "            )\n",
    "        )\n",
    "    return minimal_docs\n",
    "minimal_docs = filter_to_minimal_docs(extracted_data)\n",
    "minimal_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ef99c98",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'minimal_docs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m     texts_chunk \u001b[38;5;241m=\u001b[39m text_splitter\u001b[38;5;241m.\u001b[39msplit_documents(minimal_docs)\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m texts_chunk\n\u001b[0;32m---> 10\u001b[0m texts_chunk \u001b[38;5;241m=\u001b[39m text_split(\u001b[43mminimal_docs\u001b[49m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of chunks: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(texts_chunk)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m texts_chunk\n",
      "\u001b[0;31mNameError\u001b[0m: name 'minimal_docs' is not defined"
     ]
    }
   ],
   "source": [
    "# Split the documents into smaller chunks\n",
    "def text_split(minimal_docs):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=500,\n",
    "        chunk_overlap=20,\n",
    "    )\n",
    "    texts_chunk = text_splitter.split_documents(minimal_docs)\n",
    "    return texts_chunk\n",
    "\n",
    "texts_chunk = text_split(minimal_docs)\n",
    "print(f\"Number of chunks: {len(texts_chunk)}\")\n",
    "texts_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6855a59f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gq/fngx79cs1p34q80fpp8n4x540000gn/T/ipykernel_42713/3442840130.py:8: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(\n",
      "/Users/venkat/anaconda3/envs/medibot/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HuggingFaceEmbeddings(client=SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \n",
       "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
       "  (2): Normalize()\n",
       "), model_name='sentence-transformers/all-MiniLM-L6-v2', cache_folder=None, model_kwargs={}, encode_kwargs={}, multi_process=False, show_progress=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "def download_embeddings():\n",
    "    \"\"\"\n",
    "    Download and return the HuggingFace embeddings model.\n",
    "    \"\"\"\n",
    "    model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=model_name\n",
    "    )\n",
    "    return embeddings\n",
    "\n",
    "embedding = download_embeddings()\n",
    "embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc7fb817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy: 1.26.4\n",
      "Torch: 2.2.2\n",
      "Torch → NumPy conversion OK: (3, 4)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "print(\"NumPy:\", np.__version__)\n",
    "print(\"Torch:\", torch.__version__)\n",
    "\n",
    "# critical test\n",
    "x = torch.randn(3, 4)\n",
    "y = x.numpy()\n",
    "print(\"Torch → NumPy conversion OK:\", y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93a62409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector length: 384\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[-0.034477319568395615,\n",
       " 0.031023208051919937,\n",
       " 0.0067349751479923725,\n",
       " 0.0261089950799942,\n",
       " -0.03936201333999634,\n",
       " -0.16030244529247284,\n",
       " 0.06692400574684143,\n",
       " -0.006441435310989618,\n",
       " -0.04745043069124222,\n",
       " 0.01475885882973671,\n",
       " 0.07087526470422745,\n",
       " 0.055527638643980026,\n",
       " 0.019193362444639206,\n",
       " -0.026251330971717834,\n",
       " -0.010109556838870049,\n",
       " -0.026940438896417618,\n",
       " 0.022307435050606728,\n",
       " -0.022226642817258835,\n",
       " -0.1496925801038742,\n",
       " -0.017492979764938354,\n",
       " 0.007676245644688606,\n",
       " 0.05435226857662201,\n",
       " 0.003254440613090992,\n",
       " 0.031725868582725525,\n",
       " -0.08462143689393997,\n",
       " -0.029406003654003143,\n",
       " 0.05159556120634079,\n",
       " 0.0481240376830101,\n",
       " -0.0033148236107081175,\n",
       " -0.05827918276190758,\n",
       " 0.04196929931640625,\n",
       " 0.022210687398910522,\n",
       " 0.1281888782978058,\n",
       " -0.02233893610537052,\n",
       " -0.011656295508146286,\n",
       " 0.06292832642793655,\n",
       " -0.03287631645798683,\n",
       " -0.09122603386640549,\n",
       " -0.031175334006547928,\n",
       " 0.05269952863454819,\n",
       " 0.047034792602062225,\n",
       " -0.08420310914516449,\n",
       " -0.030056169256567955,\n",
       " -0.020744813606142998,\n",
       " 0.009517845697700977,\n",
       " -0.0037217866629362106,\n",
       " 0.007343279663473368,\n",
       " 0.039324309676885605,\n",
       " 0.09327401220798492,\n",
       " -0.0037885834462940693,\n",
       " -0.05274210125207901,\n",
       " -0.05805816873908043,\n",
       " -0.006864306982606649,\n",
       " 0.005283229984343052,\n",
       " 0.08289296925067902,\n",
       " 0.019362797960639,\n",
       " 0.006284521427005529,\n",
       " -0.01033077947795391,\n",
       " 0.009032352827489376,\n",
       " -0.037683721631765366,\n",
       " -0.045206066220998764,\n",
       " 0.024016374722123146,\n",
       " -0.0069441888481378555,\n",
       " 0.013491645455360413,\n",
       " 0.10005492717027664,\n",
       " -0.07168389856815338,\n",
       " -0.02169506624341011,\n",
       " 0.0316183902323246,\n",
       " -0.05163463577628136,\n",
       " -0.08224771916866302,\n",
       " -0.06569329649209976,\n",
       " -0.00989533495157957,\n",
       " 0.005816377699375153,\n",
       " 0.07355452328920364,\n",
       " -0.034050341695547104,\n",
       " 0.024886073544621468,\n",
       " 0.01448804885149002,\n",
       " 0.026457343250513077,\n",
       " 0.009656763635575771,\n",
       " 0.030217286199331284,\n",
       " 0.05280398204922676,\n",
       " -0.07535990327596664,\n",
       " 0.009897151030600071,\n",
       " 0.02983677200973034,\n",
       " 0.017555586993694305,\n",
       " 0.023091964423656464,\n",
       " 0.0019338926067575812,\n",
       " 0.0014001791132614017,\n",
       " -0.04717598110437393,\n",
       " -0.011194327846169472,\n",
       " -0.11420142650604248,\n",
       " -0.019811971113085747,\n",
       " 0.0402662493288517,\n",
       " 0.0021929957438260317,\n",
       " -0.07979216426610947,\n",
       " -0.025382310152053833,\n",
       " 0.09448301792144775,\n",
       " -0.028981052339076996,\n",
       " -0.14500251412391663,\n",
       " 0.2309773862361908,\n",
       " 0.02773117646574974,\n",
       " 0.03211148455739021,\n",
       " 0.031065016984939575,\n",
       " 0.04283280670642853,\n",
       " 0.06423775851726532,\n",
       " 0.03216315805912018,\n",
       " -0.004876758903264999,\n",
       " 0.0556994304060936,\n",
       " -0.037532348185777664,\n",
       " -0.021505529060959816,\n",
       " -0.028342656791210175,\n",
       " -0.028846919536590576,\n",
       " 0.03835310786962509,\n",
       " -0.017468666657805443,\n",
       " 0.05248522758483887,\n",
       " -0.07487601786851883,\n",
       " -0.031259749084711075,\n",
       " 0.021841559559106827,\n",
       " -0.039895687252283096,\n",
       " -0.008587081916630268,\n",
       " 0.026956554502248764,\n",
       " -0.048495493829250336,\n",
       " 0.011469876393675804,\n",
       " 0.02961817756295204,\n",
       " -0.02057219110429287,\n",
       " 0.013103825971484184,\n",
       " 0.02883347123861313,\n",
       " -3.1941979827962646e-33,\n",
       " 0.06478208303451538,\n",
       " -0.018130209296941757,\n",
       " 0.05178992077708244,\n",
       " 0.12198270857334137,\n",
       " 0.028780130669474602,\n",
       " 0.008721995167434216,\n",
       " -0.07052118331193924,\n",
       " -0.01690731942653656,\n",
       " 0.04073970392346382,\n",
       " 0.04211616516113281,\n",
       " 0.025447169318795204,\n",
       " 0.035746265202760696,\n",
       " -0.04914474859833717,\n",
       " 0.0021290231961756945,\n",
       " -0.015546581707894802,\n",
       " 0.050730518996715546,\n",
       " -0.04818527773022652,\n",
       " 0.03588062524795532,\n",
       " -0.00406698789447546,\n",
       " 0.10172472894191742,\n",
       " -0.055970046669244766,\n",
       " -0.010681061074137688,\n",
       " 0.011235800571739674,\n",
       " 0.09068656712770462,\n",
       " 0.0042344448156654835,\n",
       " 0.03513864427804947,\n",
       " -0.009702840819954872,\n",
       " -0.09386516362428665,\n",
       " 0.09285559505224228,\n",
       " 0.008004934526979923,\n",
       " -0.007705461699515581,\n",
       " -0.05208670347929001,\n",
       " -0.01258799433708191,\n",
       " 0.0032669678330421448,\n",
       " 0.00601351959630847,\n",
       " 0.007581581361591816,\n",
       " 0.010517138987779617,\n",
       " -0.08634558320045471,\n",
       " -0.06987879425287247,\n",
       " -0.002533883787691593,\n",
       " -0.09097657352685928,\n",
       " 0.04688729718327522,\n",
       " 0.05207652226090431,\n",
       " 0.007193864788860083,\n",
       " 0.010903660207986832,\n",
       " -0.005229497794061899,\n",
       " 0.013937306590378284,\n",
       " 0.021968362852931023,\n",
       " 0.034208618104457855,\n",
       " 0.06022470071911812,\n",
       " 0.00011667551734717563,\n",
       " 0.014731966890394688,\n",
       " -0.07008922100067139,\n",
       " 0.02849903702735901,\n",
       " -0.027601735666394234,\n",
       " 0.010768383741378784,\n",
       " 0.034830957651138306,\n",
       " -0.022487884387373924,\n",
       " 0.009769017808139324,\n",
       " 0.07722783088684082,\n",
       " 0.021588385105133057,\n",
       " 0.11495617777109146,\n",
       " -0.0680011734366417,\n",
       " 0.023760952055454254,\n",
       " -0.01598397269845009,\n",
       " -0.017826944589614868,\n",
       " 0.06439490616321564,\n",
       " 0.03202572464942932,\n",
       " 0.050270285457372665,\n",
       " -0.005913727451115847,\n",
       " -0.03370809182524681,\n",
       " 0.01784026250243187,\n",
       " 0.0165733452886343,\n",
       " 0.06329651176929474,\n",
       " 0.034677162766456604,\n",
       " 0.046473465859889984,\n",
       " 0.0979061871767044,\n",
       " -0.006635515950620174,\n",
       " 0.02520713396370411,\n",
       " -0.07798829674720764,\n",
       " 0.01692645065486431,\n",
       " -0.000945836422033608,\n",
       " 0.022471914067864418,\n",
       " -0.03825322166085243,\n",
       " 0.09570483863353729,\n",
       " -0.005350802093744278,\n",
       " 0.010469086468219757,\n",
       " -0.11524052172899246,\n",
       " -0.013262549415230751,\n",
       " -0.01070941612124443,\n",
       " -0.0831172913312912,\n",
       " 0.07327356934547424,\n",
       " 0.049392204731702805,\n",
       " -0.008994351141154766,\n",
       " -0.09584552049636841,\n",
       " 3.366147459724626e-33,\n",
       " 0.12493182718753815,\n",
       " 0.01934969611465931,\n",
       " -0.058225687593221664,\n",
       " -0.03598824888467789,\n",
       " -0.05074671283364296,\n",
       " -0.04566236585378647,\n",
       " -0.08260339498519897,\n",
       " 0.14819471538066864,\n",
       " -0.08842119574546814,\n",
       " 0.06027445197105408,\n",
       " 0.05103013664484024,\n",
       " 0.010303132236003876,\n",
       " 0.14121422171592712,\n",
       " 0.03081381693482399,\n",
       " 0.06103309616446495,\n",
       " -0.05285126343369484,\n",
       " 0.13664893805980682,\n",
       " 0.009189920499920845,\n",
       " -0.01732519268989563,\n",
       " -0.012848577462136745,\n",
       " -0.007995319552719593,\n",
       " -0.05098005011677742,\n",
       " -0.05235058441758156,\n",
       " 0.007593036629259586,\n",
       " -0.01516634039580822,\n",
       " 0.01696031726896763,\n",
       " 0.021270520985126495,\n",
       " 0.020558079704642296,\n",
       " -0.12002810090780258,\n",
       " 0.014461810700595379,\n",
       " 0.026759937405586243,\n",
       " 0.025330644100904465,\n",
       " -0.04275466501712799,\n",
       " 0.006768465507775545,\n",
       " -0.014458565041422844,\n",
       " 0.04526195302605629,\n",
       " -0.09147650003433228,\n",
       " -0.019439123570919037,\n",
       " -0.017833519726991653,\n",
       " -0.054910123348236084,\n",
       " -0.052641112357378006,\n",
       " -0.010459096170961857,\n",
       " -0.052016086876392365,\n",
       " 0.020891938358545303,\n",
       " -0.07997031509876251,\n",
       " -0.012111304327845573,\n",
       " -0.05773142725229263,\n",
       " 0.023178234696388245,\n",
       " -0.008031745441257954,\n",
       " -0.02598932385444641,\n",
       " -0.07995671033859253,\n",
       " -0.02072884701192379,\n",
       " 0.0488177128136158,\n",
       " -0.020389091223478317,\n",
       " -0.04917660355567932,\n",
       " 0.014159677550196648,\n",
       " -0.06362203508615494,\n",
       " -0.007807387970387936,\n",
       " 0.0164315328001976,\n",
       " -0.025682488456368446,\n",
       " 0.013381139375269413,\n",
       " 0.02624877355992794,\n",
       " 0.009978417307138443,\n",
       " 0.06322886794805527,\n",
       " 0.0026722026523202658,\n",
       " -0.0065827262587845325,\n",
       " 0.01663191430270672,\n",
       " 0.03236639127135277,\n",
       " 0.03794240579009056,\n",
       " -0.036376021802425385,\n",
       " -0.0069109732285141945,\n",
       " 0.00015961473400238901,\n",
       " -0.0016335288528352976,\n",
       " -0.027278130874037743,\n",
       " -0.028038084506988525,\n",
       " 0.049681469798088074,\n",
       " -0.02886722981929779,\n",
       " -0.002418067306280136,\n",
       " 0.014774891547858715,\n",
       " 0.009764570742845535,\n",
       " 0.005797651130706072,\n",
       " 0.01348612830042839,\n",
       " 0.005567918065935373,\n",
       " 0.03722710534930229,\n",
       " 0.007232470437884331,\n",
       " 0.0401562862098217,\n",
       " 0.08150327205657959,\n",
       " 0.07199165225028992,\n",
       " -0.01305614784359932,\n",
       " -0.04288206249475479,\n",
       " -0.011011254042387009,\n",
       " 0.004897788166999817,\n",
       " -0.009229691699147224,\n",
       " 0.035191476345062256,\n",
       " -0.051034990698099136,\n",
       " -1.571437557856825e-08,\n",
       " -0.08862445503473282,\n",
       " 0.023909306153655052,\n",
       " -0.016238771378993988,\n",
       " 0.03170051425695419,\n",
       " 0.02728424221277237,\n",
       " 0.052468810230493546,\n",
       " -0.047070953994989395,\n",
       " -0.058847472071647644,\n",
       " -0.0632081851363182,\n",
       " 0.04088853672146797,\n",
       " 0.04982796311378479,\n",
       " 0.10655172169208527,\n",
       " -0.07450234889984131,\n",
       " -0.012495443224906921,\n",
       " 0.018370728939771652,\n",
       " 0.03947407007217407,\n",
       " -0.024797895923256874,\n",
       " 0.014516262337565422,\n",
       " -0.037069182842969894,\n",
       " 0.02001568302512169,\n",
       " -4.858829197473824e-05,\n",
       " 0.009866568259894848,\n",
       " 0.024838777258992195,\n",
       " -0.05245814099907875,\n",
       " 0.029314152896404266,\n",
       " -0.08719193190336227,\n",
       " -0.014499749056994915,\n",
       " 0.02601909637451172,\n",
       " -0.01874634623527527,\n",
       " -0.07620515674352646,\n",
       " 0.03504331037402153,\n",
       " 0.10363952070474625,\n",
       " -0.028050456196069717,\n",
       " 0.012718145735561848,\n",
       " -0.0763254463672638,\n",
       " -0.018652327358722687,\n",
       " 0.02497672103345394,\n",
       " 0.08144529908895493,\n",
       " 0.06875885277986526,\n",
       " -0.06405660510063171,\n",
       " -0.08389382809400558,\n",
       " 0.06136234477162361,\n",
       " -0.03354555368423462,\n",
       " -0.10615337640047073,\n",
       " -0.040080565959215164,\n",
       " 0.032530173659324646,\n",
       " 0.0766248106956482,\n",
       " -0.07301616668701172,\n",
       " 0.00033755265758372843,\n",
       " -0.04087164252996445,\n",
       " -0.07578849047422409,\n",
       " 0.027527665719389915,\n",
       " 0.07462538778781891,\n",
       " 0.017717301845550537,\n",
       " 0.0912184864282608,\n",
       " 0.11022018641233444,\n",
       " 0.0005698121385648847,\n",
       " 0.05146332085132599,\n",
       " -0.014551316387951374,\n",
       " 0.033231984823942184,\n",
       " 0.023792261257767677,\n",
       " -0.02288982644677162,\n",
       " 0.0389375239610672,\n",
       " 0.0302068330347538]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector = embedding.embed_query(\"Hello world\")\n",
    "print( \"Vector length:\", len(vector))\n",
    "vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9359a5d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9908fe99",
   "metadata": {},
   "outputs": [],
   "source": [
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "HUGGINGFACEHUB_API_TOKEN = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")\n",
    "\n",
    "\n",
    "os.environ[\"PINECONE_API_KEY\"] = PINECONE_API_KEY\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = HUGGINGFACEHUB_API_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d4e9bda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pinecone.pinecone.Pinecone at 0x1283abf70>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pinecone import Pinecone \n",
    "pinecone_api_key = PINECONE_API_KEY\n",
    "\n",
    "pc = Pinecone(api_key=pinecone_api_key)\n",
    "pc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d570301",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import ServerlessSpec \n",
    "\n",
    "index_name = \"medical-chatbot\"\n",
    "\n",
    "if not pc.has_index(index_name):\n",
    "    pc.create_index(\n",
    "        name = index_name,\n",
    "        dimension=384,  # Dimension of the embeddings\n",
    "        metric= \"cosine\",  # Cosine similarity\n",
    "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")\n",
    "    )\n",
    "\n",
    "\n",
    "index = pc.Index(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11df048",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "# docsearch = PineconeVectorStore.from_documents(\n",
    "#     documents=texts_chunk,\n",
    "#     embedding=embedding,\n",
    "#     index_name=index_name\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd2a960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Existing index \n",
    "\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "# # Embed each chunk and upsert the embeddings into your Pinecone index.\n",
    "# docsearch = PineconeVectorStore.from_existing_index(\n",
    "#     index_name=index_name,\n",
    "#     embedding=embedding\n",
    "# )\n",
    "\n",
    "docsearch = PineconeVectorStore(\n",
    "    index=index,\n",
    "    embedding=embedding,   # SAME embedding model you used earlier\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "74c4dc1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='8b41d91f-0677-4f14-9cd3-aeb4f76391b3', metadata={'source': 'data/Medical_book.pdf'}, page_content='GALE ENCYCLOPEDIA OF MEDICINE 226\\nAcne\\nGEM - 0001 to 0432 - A  10/22/03 1:41 PM  Page 26'),\n",
       " Document(id='70eb69a8-4db1-42eb-93a4-9ffada4464ac', metadata={'source': 'data/Medical_book.pdf'}, page_content='GALE ENCYCLOPEDIA OF MEDICINE 226\\nAcne\\nGEM - 0001 to 0432 - A  10/22/03 1:41 PM  Page 26'),\n",
       " Document(id='18ef835c-d197-476d-baff-da5e0f3e8b3f', metadata={'source': 'data/Medical_book.pdf'}, page_content='GALE ENCYCLOPEDIA OF MEDICINE 2 25\\nAcne\\nAcne vulgaris affecting a woman’s face. Acne is the general\\nname given to a skin disorder in which the sebaceous\\nglands become inflamed. (Photograph by Biophoto Associ-\\nates, Photo Researchers, Inc. Reproduced by permission.)\\nGEM - 0001 to 0432 - A  10/22/03 1:41 PM  Page 25')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = docsearch.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":3})\n",
    "retrieved_docs = retriever.invoke(\"What is Acne?\")\n",
    "retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae126a7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /Users/venkat/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "# # from langchain_openai import ChatOpenAI\n",
    "\n",
    "# # chatModel = ChatOpenAI(model=\"gpt-4o\")\n",
    "\n",
    "# # from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# # # chatModel = ChatGoogleGenerativeAI(\n",
    "# # #     model=\"gemini-1.5-flash\",\n",
    "# # #     temperature=0,\n",
    "# # # )\n",
    "\n",
    "\n",
    "# # chatModel = ChatGoogleGenerativeAI(\n",
    "# #     model=\"gemini-2.0-flash\",\n",
    "# #     temperature=0,\n",
    "# # )\n",
    "\n",
    "# from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
    "\n",
    "# # chatModel = HuggingFaceEndpoint(\n",
    "# #     repo_id=\"tiiuae/falcon-7b-instruct\",  # good free model\n",
    "# #     max_length=512,\n",
    "# #     temperature=0.3,\n",
    "# # )\n",
    "\n",
    "# chatModel = HuggingFaceEndpoint(\n",
    "#     repo_id='TheBloke/guanaco-7B-HF',\n",
    "#     task='text-generation',\n",
    "#     temperature=0.3,\n",
    "#     max_new_tokens=512,\n",
    "#     huggingfacehub_api_token=os.getenv(\"HUGGINGFACEHUB_API_TOKEN\"),\n",
    "# )\n",
    "# # chatModel = HuggingFaceEndpoint(\n",
    "# # repo_id='meta-llama/Llama-2-7b-chat-hf', # Replace as needed\n",
    "# # task='text-generation',\n",
    "# # temperature=0.7,\n",
    "# # max_new_tokens=200\n",
    "# # )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4d48cfcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/venkat/anaconda3/envs/medibot/lib/python3.10/site-packages/pydantic/_internal/_generate_schema.py:648: ArbitraryTypeWarning: <built-in function any> is not a Python type (it may be an instance of an object), Pydantic will allow any object with no validation since we cannot even enforce that the input is an instance of the given type. To get rid of this error wrap the type with `pydantic.SkipValidation`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Using `low_cpu_mem_usage=True` or a `device_map` requires Accelerate: `pip install accelerate`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[61], line 49\u001b[0m\n\u001b[1;32m     46\u001b[0m model_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTinyLlama/TinyLlama-1.1B-Chat-v1.0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     48\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_id)\n\u001b[0;32m---> 49\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     53\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m hf_pipeline \u001b[38;5;241m=\u001b[39m pipeline(\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext-generation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     57\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     58\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer\n\u001b[1;32m     59\u001b[0m )\n\u001b[1;32m     61\u001b[0m chatModel \u001b[38;5;241m=\u001b[39m LocalHFLLM(pipeline\u001b[38;5;241m=\u001b[39mhf_pipeline)\n",
      "File \u001b[0;32m~/anaconda3/envs/medibot/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:563\u001b[0m, in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    560\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[1;32m    561\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m    562\u001b[0m         pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39mmodel_args, config\u001b[38;5;241m=\u001b[39mconfig, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m--> 563\u001b[0m     )\n\u001b[1;32m    564\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    565\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    566\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    567\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/medibot/lib/python3.10/site-packages/transformers/modeling_utils.py:3122\u001b[0m, in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3113\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m   3114\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError no file named \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_add_variant(SAFE_WEIGHTS_NAME,\u001b[38;5;250m \u001b[39mvariant)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m found in directory\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3115\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3116\u001b[0m         )\n\u001b[1;32m   3117\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3118\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m   3119\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError no file named \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_add_variant(WEIGHTS_NAME,\u001b[38;5;250m \u001b[39mvariant)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mTF2_WEIGHTS_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3120\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mTF_WEIGHTS_NAME\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.index\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m or \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mFLAX_WEIGHTS_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m found in directory\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3121\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 3122\u001b[0m         )\n\u001b[1;32m   3123\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(subfolder, pretrained_model_name_or_path)):\n\u001b[1;32m   3124\u001b[0m     archive_file \u001b[38;5;241m=\u001b[39m pretrained_model_name_or_path\n",
      "\u001b[0;31mImportError\u001b[0m: Using `low_cpu_mem_usage=True` or a `device_map` requires Accelerate: `pip install accelerate`"
     ]
    }
   ],
   "source": [
    "# from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "# from langchain_huggingface import HuggingFacePipeline\n",
    "\n",
    "# model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_id,\n",
    "#     device_map=\"auto\",        # CPU or GPU\n",
    "#     torch_dtype=\"auto\"\n",
    "# )\n",
    "\n",
    "# text_gen_pipeline = pipeline(\n",
    "#     \"text-generation\",\n",
    "#     model=model,\n",
    "#     tokenizer=tokenizer,\n",
    "#     max_new_tokens=256,\n",
    "#     temperature=0.2,\n",
    "#     do_sample=True\n",
    "# )\n",
    "\n",
    "# chatModel = HuggingFacePipeline(pipeline=text_gen_pipeline)\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from langchain_core.language_models.llms import LLM\n",
    "from typing import Optional, List\n",
    "import torch\n",
    "\n",
    "class LocalHFLLM(LLM):\n",
    "    pipeline: any\n",
    "\n",
    "    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
    "        output = self.pipeline(\n",
    "            prompt,\n",
    "            max_new_tokens=256,\n",
    "            temperature=0.2,\n",
    "            do_sample=True,\n",
    "        )[0][\"generated_text\"]\n",
    "        return output[len(prompt):]\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"local_hf\"\n",
    "\n",
    "\n",
    "model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=\"auto\"\n",
    ")\n",
    "\n",
    "hf_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "chatModel = LocalHFLLM(pipeline=hf_pipeline)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbdd459",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n"
     ]
    },
    {
     "ename": "HfHubHTTPError",
     "evalue": "410 Client Error: Gone for url: https://api-inference.huggingface.co/models/TheBloke/guanaco-7B-HF/v1/chat/completions\n\nhttps://api-inference.huggingface.co is no longer supported. Please use https://router.huggingface.co instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/medibot/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py:304\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 304\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/envs/medibot/lib/python3.10/site-packages/requests/models.py:1026\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1026\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 410 Client Error: Gone for url: https://api-inference.huggingface.co/models/TheBloke/guanaco-7B-HF/v1/chat/completions",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mHfHubHTTPError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m ChatHuggingFace(llm\u001b[38;5;241m=\u001b[39mchatModel)\n\u001b[0;32m----> 3\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhat is the capital of India?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(result\u001b[38;5;241m.\u001b[39mcontent)\n",
      "File \u001b[0;32m~/anaconda3/envs/medibot/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:395\u001b[0m, in \u001b[0;36minvoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    382\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    383\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid input type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(model_input)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    384\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMust be a PromptValue, str, or list of BaseMessages.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    385\u001b[0m     )\n\u001b[1;32m    386\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m    388\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    390\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    391\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[1;32m    392\u001b[0m     config: RunnableConfig \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    393\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m    394\u001b[0m     stop: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m--> 395\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    396\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m AIMessage:\n\u001b[1;32m    397\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    398\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[1;32m    399\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAIMessage\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    400\u001b[0m         cast(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    412\u001b[0m         )\u001b[38;5;241m.\u001b[39mmessage,\n\u001b[1;32m    413\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/medibot/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:1025\u001b[0m, in \u001b[0;36mgenerate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "File \u001b[0;32m~/anaconda3/envs/medibot/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:842\u001b[0m, in \u001b[0;36mgenerate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    839\u001b[0m     params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs}\n\u001b[1;32m    840\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28msorted\u001b[39m(params\u001b[38;5;241m.\u001b[39mitems()))\n\u001b[0;32m--> 842\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgenerate\u001b[39m(\n\u001b[1;32m    843\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    844\u001b[0m     messages: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mlist\u001b[39m[BaseMessage]],\n\u001b[1;32m    845\u001b[0m     stop: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    846\u001b[0m     callbacks: Callbacks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    847\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m    848\u001b[0m     tags: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    849\u001b[0m     metadata: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    850\u001b[0m     run_name: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    851\u001b[0m     run_id: uuid\u001b[38;5;241m.\u001b[39mUUID \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    852\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    853\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    854\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Pass a sequence of prompts to the model and return model generations.\u001b[39;00m\n\u001b[1;32m    855\u001b[0m \n\u001b[1;32m    856\u001b[0m \u001b[38;5;124;03m    This method should make use of batched calls for models that expose a batched\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    887\u001b[0m \n\u001b[1;32m    888\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    889\u001b[0m     ls_structured_output_format \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\n\u001b[1;32m    890\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mls_structured_output_format\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    891\u001b[0m     ) \u001b[38;5;129;01mor\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstructured_output_format\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/medibot/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:1091\u001b[0m, in \u001b[0;36m_generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1078\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\n\u001b[1;32m   1079\u001b[0m             \u001b[38;5;241m*\u001b[39m[\n\u001b[1;32m   1080\u001b[0m                 run_manager\u001b[38;5;241m.\u001b[39mon_llm_end(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1088\u001b[0m             ]\n\u001b[1;32m   1089\u001b[0m         )\n\u001b[1;32m   1090\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m-> 1091\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m   1092\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[1;32m   1093\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[1;32m   1094\u001b[0m ]\n\u001b[1;32m   1095\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[1;32m   1096\u001b[0m generations \u001b[38;5;241m=\u001b[39m [res\u001b[38;5;241m.\u001b[39mgenerations \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results]  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/medibot/lib/python3.10/site-packages/langchain_huggingface/chat_models/huggingface.py:370\u001b[0m, in \u001b[0;36m_generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mChatHuggingFace\u001b[39;00m(BaseChatModel):\n\u001b[1;32m    325\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Hugging Face LLM's as ChatModels.\u001b[39;00m\n\u001b[1;32m    326\u001b[0m \n\u001b[1;32m    327\u001b[0m \u001b[38;5;124;03m    Works with `HuggingFaceTextGenInference`, `HuggingFaceEndpoint`,\u001b[39;00m\n\u001b[1;32m    328\u001b[0m \u001b[38;5;124;03m    `HuggingFaceHub`, and `HuggingFacePipeline` LLMs.\u001b[39;00m\n\u001b[1;32m    329\u001b[0m \n\u001b[1;32m    330\u001b[0m \u001b[38;5;124;03m    Upon instantiating this class, the model_id is resolved from the url\u001b[39;00m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;124;03m    provided to the LLM, and the appropriate tokenizer is loaded from\u001b[39;00m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;124;03m    the HuggingFace Hub.\u001b[39;00m\n\u001b[1;32m    333\u001b[0m \n\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03m    Setup:\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \u001b[38;5;124;03m        Install `langchain-huggingface` and ensure your Hugging Face token\u001b[39;00m\n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03m        is saved.\u001b[39;00m\n\u001b[1;32m    337\u001b[0m \n\u001b[1;32m    338\u001b[0m \u001b[38;5;124;03m        ```bash\u001b[39;00m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;124;03m        pip install langchain-huggingface\u001b[39;00m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;124;03m        ```\u001b[39;00m\n\u001b[1;32m    341\u001b[0m \n\u001b[1;32m    342\u001b[0m \u001b[38;5;124;03m        ```python\u001b[39;00m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;124;03m        from huggingface_hub import login\u001b[39;00m\n\u001b[1;32m    344\u001b[0m \n\u001b[1;32m    345\u001b[0m \u001b[38;5;124;03m        login()  # You will be prompted for your HF key, which will then be saved locally\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;124;03m        ```\u001b[39;00m\n\u001b[1;32m    347\u001b[0m \n\u001b[1;32m    348\u001b[0m \u001b[38;5;124;03m    Key init args — completion params:\u001b[39;00m\n\u001b[1;32m    349\u001b[0m \u001b[38;5;124;03m        llm:\u001b[39;00m\n\u001b[1;32m    350\u001b[0m \u001b[38;5;124;03m            LLM to be used.\u001b[39;00m\n\u001b[1;32m    351\u001b[0m \n\u001b[1;32m    352\u001b[0m \u001b[38;5;124;03m    Key init args — client params:\u001b[39;00m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;124;03m        custom_get_token_ids:\u001b[39;00m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;124;03m            Optional encoder to use for counting tokens.\u001b[39;00m\n\u001b[1;32m    355\u001b[0m \u001b[38;5;124;03m        metadata:\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;124;03m            Metadata to add to the run trace.\u001b[39;00m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;124;03m        tags:\u001b[39;00m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;124;03m            Tags to add to the run trace.\u001b[39;00m\n\u001b[1;32m    359\u001b[0m \u001b[38;5;124;03m        verbose:\u001b[39;00m\n\u001b[1;32m    360\u001b[0m \u001b[38;5;124;03m            Whether to print out response text.\u001b[39;00m\n\u001b[1;32m    361\u001b[0m \n\u001b[1;32m    362\u001b[0m \u001b[38;5;124;03m    See full list of supported init args and their descriptions in the params\u001b[39;00m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;124;03m    section.\u001b[39;00m\n\u001b[1;32m    364\u001b[0m \n\u001b[1;32m    365\u001b[0m \u001b[38;5;124;03m    Instantiate:\u001b[39;00m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;124;03m        ```python\u001b[39;00m\n\u001b[1;32m    367\u001b[0m \u001b[38;5;124;03m        from langchain_huggingface import HuggingFaceEndpoint,\u001b[39;00m\n\u001b[1;32m    368\u001b[0m \u001b[38;5;124;03m        ChatHuggingFace\u001b[39;00m\n\u001b[1;32m    369\u001b[0m \n\u001b[0;32m--> 370\u001b[0m \u001b[38;5;124;03m        model = HuggingFaceEndpoint(\u001b[39;00m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;124;03m            repo_id=\"microsoft/Phi-3-mini-4k-instruct\",\u001b[39;00m\n\u001b[1;32m    372\u001b[0m \u001b[38;5;124;03m            task=\"text-generation\",\u001b[39;00m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;124;03m            max_new_tokens=512,\u001b[39;00m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;124;03m            do_sample=False,\u001b[39;00m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;124;03m            repetition_penalty=1.03,\u001b[39;00m\n\u001b[1;32m    376\u001b[0m \u001b[38;5;124;03m        )\u001b[39;00m\n\u001b[1;32m    377\u001b[0m \n\u001b[1;32m    378\u001b[0m \u001b[38;5;124;03m        chat = ChatHuggingFace(llm=model, verbose=True)\u001b[39;00m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;124;03m        ```\u001b[39;00m\n\u001b[1;32m    380\u001b[0m \n\u001b[1;32m    381\u001b[0m \u001b[38;5;124;03m    Invoke:\u001b[39;00m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;124;03m        ```python\u001b[39;00m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;124;03m        messages = [\u001b[39;00m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;124;03m            (\"system\", \"You are a helpful translator. Translate the user\u001b[39;00m\n\u001b[1;32m    385\u001b[0m \u001b[38;5;124;03m            sentence to French.\"),\u001b[39;00m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;124;03m            (\"human\", \"I love programming.\"),\u001b[39;00m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;124;03m        ]\u001b[39;00m\n\u001b[1;32m    388\u001b[0m \n\u001b[1;32m    389\u001b[0m \u001b[38;5;124;03m        chat(...).invoke(messages)\u001b[39;00m\n\u001b[1;32m    390\u001b[0m \u001b[38;5;124;03m        ```\u001b[39;00m\n\u001b[1;32m    391\u001b[0m \n\u001b[1;32m    392\u001b[0m \u001b[38;5;124;03m        ```python\u001b[39;00m\n\u001b[1;32m    393\u001b[0m \u001b[38;5;124;03m        AIMessage(content='Je ai une passion pour le programme.\\n\\nIn\u001b[39;00m\n\u001b[1;32m    394\u001b[0m \u001b[38;5;124;03m        French, we use \"ai\" for masculine subjects and \"a\" for feminine\u001b[39;00m\n\u001b[1;32m    395\u001b[0m \u001b[38;5;124;03m        subjects. Since \"programming\" is gender-neutral in English, we\u001b[39;00m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;124;03m        will go with the masculine \"programme\".\\n\\nConfirmation: \"J\\'aime\u001b[39;00m\n\u001b[1;32m    397\u001b[0m \u001b[38;5;124;03m        le programme.\" is more commonly used. The sentence above is\u001b[39;00m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;124;03m        technically accurate, but less commonly used in spoken French as\u001b[39;00m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;124;03m        \"ai\" is used less frequently in everyday speech.',\u001b[39;00m\n\u001b[1;32m    400\u001b[0m \u001b[38;5;124;03m        response_metadata={'token_usage': ChatCompletionOutputUsage\u001b[39;00m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;124;03m        (completion_tokens=100, prompt_tokens=55, total_tokens=155),\u001b[39;00m\n\u001b[1;32m    402\u001b[0m \u001b[38;5;124;03m        'model': '', 'finish_reason': 'length'},\u001b[39;00m\n\u001b[1;32m    403\u001b[0m \u001b[38;5;124;03m        id='run-874c24b7-0272-4c99-b259-5d6d7facbc56-0')\u001b[39;00m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;124;03m        ```\u001b[39;00m\n\u001b[1;32m    405\u001b[0m \n\u001b[1;32m    406\u001b[0m \u001b[38;5;124;03m    Stream:\u001b[39;00m\n\u001b[1;32m    407\u001b[0m \u001b[38;5;124;03m        ```python\u001b[39;00m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;124;03m        for chunk in chat.stream(messages):\u001b[39;00m\n\u001b[1;32m    409\u001b[0m \u001b[38;5;124;03m            print(chunk)\u001b[39;00m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;124;03m        ```\u001b[39;00m\n\u001b[1;32m    411\u001b[0m \n\u001b[1;32m    412\u001b[0m \u001b[38;5;124;03m        ```python\u001b[39;00m\n\u001b[1;32m    413\u001b[0m \u001b[38;5;124;03m        content='Je ai une passion pour le programme.\\n\\nIn French, we use\u001b[39;00m\n\u001b[1;32m    414\u001b[0m \u001b[38;5;124;03m        \"ai\" for masculine subjects and \"a\" for feminine subjects.\u001b[39;00m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;124;03m        Since \"programming\" is gender-neutral in English,\u001b[39;00m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;124;03m        we will go with the masculine \"programme\".\\n\\nConfirmation:\u001b[39;00m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;124;03m        \"J\\'aime le programme.\" is more commonly used. The sentence\u001b[39;00m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;124;03m        above is technically accurate, but less commonly used in spoken\u001b[39;00m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;124;03m        French as \"ai\" is used less frequently in everyday speech.'\u001b[39;00m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;124;03m        response_metadata={'token_usage': ChatCompletionOutputUsage\u001b[39;00m\n\u001b[1;32m    421\u001b[0m \u001b[38;5;124;03m        (completion_tokens=100, prompt_tokens=55, total_tokens=155),\u001b[39;00m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;124;03m        'model': '', 'finish_reason': 'length'}\u001b[39;00m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;124;03m        id='run-7d7b1967-9612-4f9a-911a-b2b5ca85046a-0'\u001b[39;00m\n\u001b[1;32m    424\u001b[0m \u001b[38;5;124;03m        ```\u001b[39;00m\n\u001b[1;32m    425\u001b[0m \n\u001b[1;32m    426\u001b[0m \u001b[38;5;124;03m    Async:\u001b[39;00m\n\u001b[1;32m    427\u001b[0m \u001b[38;5;124;03m        ```python\u001b[39;00m\n\u001b[1;32m    428\u001b[0m \u001b[38;5;124;03m        await chat.ainvoke(messages)\u001b[39;00m\n\u001b[1;32m    429\u001b[0m \u001b[38;5;124;03m        ```\u001b[39;00m\n\u001b[1;32m    430\u001b[0m \n\u001b[1;32m    431\u001b[0m \u001b[38;5;124;03m        ```python\u001b[39;00m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;124;03m        AIMessage(content='Je déaime le programming.\\n\\nLittérale : Je\u001b[39;00m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;124;03m        (j\\'aime) déaime (le) programming.\\n\\nNote: \"Programming\" in\u001b[39;00m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;124;03m        French is \"programmation\". But here, I used \"programming\" instead\u001b[39;00m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;124;03m        of \"programmation\" because the user said \"I love programming\"\u001b[39;00m\n\u001b[1;32m    436\u001b[0m \u001b[38;5;124;03m        instead of \"I love programming (in French)\", which would be\u001b[39;00m\n\u001b[1;32m    437\u001b[0m \u001b[38;5;124;03m        \"J\\'aime la programmation\". By translating the sentence\u001b[39;00m\n\u001b[1;32m    438\u001b[0m \u001b[38;5;124;03m        literally, I preserved the original meaning of the user\\'s\u001b[39;00m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;124;03m        sentence.', id='run-fd850318-e299-4735-b4c6-3496dc930b1d-0')\u001b[39;00m\n\u001b[1;32m    440\u001b[0m \u001b[38;5;124;03m        ```\u001b[39;00m\n\u001b[1;32m    441\u001b[0m \n\u001b[1;32m    442\u001b[0m \u001b[38;5;124;03m    Tool calling:\u001b[39;00m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;124;03m        ```python\u001b[39;00m\n\u001b[1;32m    444\u001b[0m \u001b[38;5;124;03m        from pydantic import BaseModel, Field\u001b[39;00m\n\u001b[1;32m    445\u001b[0m \n\u001b[1;32m    446\u001b[0m \u001b[38;5;124;03m        class GetWeather(BaseModel):\u001b[39;00m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;124;03m            '''Get the current weather in a given location'''\u001b[39;00m\n\u001b[1;32m    448\u001b[0m \n\u001b[1;32m    449\u001b[0m \u001b[38;5;124;03m            location: str = Field(..., description=\"The city and state,\u001b[39;00m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;124;03m            e.g. San Francisco, CA\")\u001b[39;00m\n\u001b[1;32m    451\u001b[0m \n\u001b[1;32m    452\u001b[0m \u001b[38;5;124;03m        class GetPopulation(BaseModel):\u001b[39;00m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;124;03m            '''Get the current population in a given location'''\u001b[39;00m\n\u001b[1;32m    454\u001b[0m \n\u001b[1;32m    455\u001b[0m \u001b[38;5;124;03m            location: str = Field(..., description=\"The city and state,\u001b[39;00m\n\u001b[1;32m    456\u001b[0m \u001b[38;5;124;03m            e.g. San Francisco, CA\")\u001b[39;00m\n\u001b[1;32m    457\u001b[0m \n\u001b[1;32m    458\u001b[0m \u001b[38;5;124;03m        chat_with_tools = chat.bind_tools([GetWeather, GetPopulation])\u001b[39;00m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;124;03m        ai_msg = chat_with_tools.invoke(\"Which city is hotter today and\u001b[39;00m\n\u001b[1;32m    460\u001b[0m \u001b[38;5;124;03m        which is bigger: LA or NY?\")\u001b[39;00m\n\u001b[1;32m    461\u001b[0m \u001b[38;5;124;03m        ai_msg.tool_calls\u001b[39;00m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;124;03m        ```\u001b[39;00m\n\u001b[1;32m    463\u001b[0m \n\u001b[1;32m    464\u001b[0m \u001b[38;5;124;03m        ```python\u001b[39;00m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;124;03m        [\u001b[39;00m\n\u001b[1;32m    466\u001b[0m \u001b[38;5;124;03m            {\u001b[39;00m\n\u001b[1;32m    467\u001b[0m \u001b[38;5;124;03m                \"name\": \"GetPopulation\",\u001b[39;00m\n\u001b[1;32m    468\u001b[0m \u001b[38;5;124;03m                \"args\": {\"location\": \"Los Angeles, CA\"},\u001b[39;00m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;124;03m                \"id\": \"0\",\u001b[39;00m\n\u001b[1;32m    470\u001b[0m \u001b[38;5;124;03m            }\u001b[39;00m\n\u001b[1;32m    471\u001b[0m \u001b[38;5;124;03m        ]\u001b[39;00m\n\u001b[1;32m    472\u001b[0m \u001b[38;5;124;03m        ```\u001b[39;00m\n\u001b[1;32m    473\u001b[0m \n\u001b[1;32m    474\u001b[0m \u001b[38;5;124;03m    Response metadata\u001b[39;00m\n\u001b[1;32m    475\u001b[0m \u001b[38;5;124;03m        ```python\u001b[39;00m\n\u001b[1;32m    476\u001b[0m \u001b[38;5;124;03m        ai_msg = chat.invoke(messages)\u001b[39;00m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;124;03m        ai_msg.response_metadata\u001b[39;00m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;124;03m        ```\u001b[39;00m\n\u001b[1;32m    479\u001b[0m \n\u001b[1;32m    480\u001b[0m \u001b[38;5;124;03m        ```python\u001b[39;00m\n\u001b[1;32m    481\u001b[0m \u001b[38;5;124;03m        {\u001b[39;00m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;124;03m            \"token_usage\": ChatCompletionOutputUsage(\u001b[39;00m\n\u001b[1;32m    483\u001b[0m \u001b[38;5;124;03m                completion_tokens=100, prompt_tokens=8, total_tokens=108\u001b[39;00m\n\u001b[1;32m    484\u001b[0m \u001b[38;5;124;03m            ),\u001b[39;00m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;124;03m            \"model\": \"\",\u001b[39;00m\n\u001b[1;32m    486\u001b[0m \u001b[38;5;124;03m            \"finish_reason\": \"length\",\u001b[39;00m\n\u001b[1;32m    487\u001b[0m \u001b[38;5;124;03m        }\u001b[39;00m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;124;03m        ```\u001b[39;00m\n\u001b[1;32m    489\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: E501\u001b[39;00m\n\u001b[1;32m    491\u001b[0m     llm: Any\n\u001b[1;32m    492\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"LLM, must be of type HuggingFaceTextGenInference, HuggingFaceEndpoint,\u001b[39;00m\n\u001b[1;32m    493\u001b[0m \u001b[38;5;124;03m        HuggingFaceHub, or HuggingFacePipeline.\"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/medibot/lib/python3.10/site-packages/huggingface_hub/inference/_client.py:706\u001b[0m, in \u001b[0;36mchat_completion\u001b[0;34m(self, messages, model, stream, frequency_penalty, logit_bias, logprobs, max_tokens, n, presence_penalty, seed, stop, temperature, tool_choice, tool_prompt, tools, top_logprobs, top_p)\u001b[0m\n\u001b[1;32m    520\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mchat_completion\u001b[39m(\n\u001b[1;32m    521\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    522\u001b[0m     messages: List[Union[Dict, ChatCompletionInputMessage]],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    543\u001b[0m     extra_body: Optional[Dict] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    544\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[ChatCompletionOutput, Iterable[ChatCompletionStreamOutput]]:\n\u001b[1;32m    545\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    546\u001b[0m \u001b[38;5;124;03m    A method for completing conversations using a specified language model.\u001b[39;00m\n\u001b[1;32m    547\u001b[0m \n\u001b[1;32m    548\u001b[0m \u001b[38;5;124;03m    > [!TIP]\u001b[39;00m\n\u001b[1;32m    549\u001b[0m \u001b[38;5;124;03m    > The `client.chat_completion` method is aliased as `client.chat.completions.create` for compatibility with OpenAI's client.\u001b[39;00m\n\u001b[1;32m    550\u001b[0m \u001b[38;5;124;03m    > Inputs and outputs are strictly the same and using either syntax will yield the same results.\u001b[39;00m\n\u001b[1;32m    551\u001b[0m \u001b[38;5;124;03m    > Check out the [Inference guide](https://huggingface.co/docs/huggingface_hub/guides/inference#openai-compatibility)\u001b[39;00m\n\u001b[1;32m    552\u001b[0m \u001b[38;5;124;03m    > for more details about OpenAI's compatibility.\u001b[39;00m\n\u001b[1;32m    553\u001b[0m \n\u001b[1;32m    554\u001b[0m \u001b[38;5;124;03m    > [!TIP]\u001b[39;00m\n\u001b[1;32m    555\u001b[0m \u001b[38;5;124;03m    > You can pass provider-specific parameters to the model by using the `extra_body` argument.\u001b[39;00m\n\u001b[1;32m    556\u001b[0m \n\u001b[1;32m    557\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    558\u001b[0m \u001b[38;5;124;03m        messages (List of [`ChatCompletionInputMessage`]):\u001b[39;00m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;124;03m            Conversation history consisting of roles and content pairs.\u001b[39;00m\n\u001b[1;32m    560\u001b[0m \u001b[38;5;124;03m        model (`str`, *optional*):\u001b[39;00m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;124;03m            The model to use for chat-completion. Can be a model ID hosted on the Hugging Face Hub or a URL to a deployed\u001b[39;00m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;124;03m            Inference Endpoint. If not provided, the default recommended model for chat-based text-generation will be used.\u001b[39;00m\n\u001b[1;32m    563\u001b[0m \u001b[38;5;124;03m            See https://huggingface.co/tasks/text-generation for more details.\u001b[39;00m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;124;03m            If `model` is a model ID, it is passed to the server as the `model` parameter. If you want to define a\u001b[39;00m\n\u001b[1;32m    565\u001b[0m \u001b[38;5;124;03m            custom URL while setting `model` in the request payload, you must set `base_url` when initializing [`InferenceClient`].\u001b[39;00m\n\u001b[1;32m    566\u001b[0m \u001b[38;5;124;03m        frequency_penalty (`float`, *optional*):\u001b[39;00m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;124;03m            Penalizes new tokens based on their existing frequency\u001b[39;00m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;124;03m            in the text so far. Range: [-2.0, 2.0]. Defaults to 0.0.\u001b[39;00m\n\u001b[1;32m    569\u001b[0m \u001b[38;5;124;03m        logit_bias (`List[float]`, *optional*):\u001b[39;00m\n\u001b[1;32m    570\u001b[0m \u001b[38;5;124;03m            Adjusts the likelihood of specific tokens appearing in the generated output.\u001b[39;00m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;124;03m        logprobs (`bool`, *optional*):\u001b[39;00m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;124;03m            Whether to return log probabilities of the output tokens or not. If true, returns the log\u001b[39;00m\n\u001b[1;32m    573\u001b[0m \u001b[38;5;124;03m            probabilities of each output token returned in the content of message.\u001b[39;00m\n\u001b[1;32m    574\u001b[0m \u001b[38;5;124;03m        max_tokens (`int`, *optional*):\u001b[39;00m\n\u001b[1;32m    575\u001b[0m \u001b[38;5;124;03m            Maximum number of tokens allowed in the response. Defaults to 100.\u001b[39;00m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;124;03m        n (`int`, *optional*):\u001b[39;00m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;124;03m            The number of completions to generate for each prompt.\u001b[39;00m\n\u001b[1;32m    578\u001b[0m \u001b[38;5;124;03m        presence_penalty (`float`, *optional*):\u001b[39;00m\n\u001b[1;32m    579\u001b[0m \u001b[38;5;124;03m            Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the\u001b[39;00m\n\u001b[1;32m    580\u001b[0m \u001b[38;5;124;03m            text so far, increasing the model's likelihood to talk about new topics.\u001b[39;00m\n\u001b[1;32m    581\u001b[0m \u001b[38;5;124;03m        response_format ([`ChatCompletionInputGrammarType`], *optional*):\u001b[39;00m\n\u001b[1;32m    582\u001b[0m \u001b[38;5;124;03m            Grammar constraints. Can be either a JSONSchema or a regex.\u001b[39;00m\n\u001b[1;32m    583\u001b[0m \u001b[38;5;124;03m        seed (Optional[`int`], *optional*):\u001b[39;00m\n\u001b[1;32m    584\u001b[0m \u001b[38;5;124;03m            Seed for reproducible control flow. Defaults to None.\u001b[39;00m\n\u001b[1;32m    585\u001b[0m \u001b[38;5;124;03m        stop (`List[str]`, *optional*):\u001b[39;00m\n\u001b[1;32m    586\u001b[0m \u001b[38;5;124;03m            Up to four strings which trigger the end of the response.\u001b[39;00m\n\u001b[1;32m    587\u001b[0m \u001b[38;5;124;03m            Defaults to None.\u001b[39;00m\n\u001b[1;32m    588\u001b[0m \u001b[38;5;124;03m        stream (`bool`, *optional*):\u001b[39;00m\n\u001b[1;32m    589\u001b[0m \u001b[38;5;124;03m            Enable realtime streaming of responses. Defaults to False.\u001b[39;00m\n\u001b[1;32m    590\u001b[0m \u001b[38;5;124;03m        stream_options ([`ChatCompletionInputStreamOptions`], *optional*):\u001b[39;00m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;124;03m            Options for streaming completions.\u001b[39;00m\n\u001b[1;32m    592\u001b[0m \u001b[38;5;124;03m        temperature (`float`, *optional*):\u001b[39;00m\n\u001b[1;32m    593\u001b[0m \u001b[38;5;124;03m            Controls randomness of the generations. Lower values ensure\u001b[39;00m\n\u001b[1;32m    594\u001b[0m \u001b[38;5;124;03m            less random completions. Range: [0, 2]. Defaults to 1.0.\u001b[39;00m\n\u001b[1;32m    595\u001b[0m \u001b[38;5;124;03m        top_logprobs (`int`, *optional*):\u001b[39;00m\n\u001b[1;32m    596\u001b[0m \u001b[38;5;124;03m            An integer between 0 and 5 specifying the number of most likely tokens to return at each token\u001b[39;00m\n\u001b[1;32m    597\u001b[0m \u001b[38;5;124;03m            position, each with an associated log probability. logprobs must be set to true if this parameter is\u001b[39;00m\n\u001b[1;32m    598\u001b[0m \u001b[38;5;124;03m            used.\u001b[39;00m\n\u001b[1;32m    599\u001b[0m \u001b[38;5;124;03m        top_p (`float`, *optional*):\u001b[39;00m\n\u001b[1;32m    600\u001b[0m \u001b[38;5;124;03m            Fraction of the most likely next words to sample from.\u001b[39;00m\n\u001b[1;32m    601\u001b[0m \u001b[38;5;124;03m            Must be between 0 and 1. Defaults to 1.0.\u001b[39;00m\n\u001b[1;32m    602\u001b[0m \u001b[38;5;124;03m        tool_choice ([`ChatCompletionInputToolChoiceClass`] or [`ChatCompletionInputToolChoiceEnum`], *optional*):\u001b[39;00m\n\u001b[1;32m    603\u001b[0m \u001b[38;5;124;03m            The tool to use for the completion. Defaults to \"auto\".\u001b[39;00m\n\u001b[1;32m    604\u001b[0m \u001b[38;5;124;03m        tool_prompt (`str`, *optional*):\u001b[39;00m\n\u001b[1;32m    605\u001b[0m \u001b[38;5;124;03m            A prompt to be appended before the tools.\u001b[39;00m\n\u001b[1;32m    606\u001b[0m \u001b[38;5;124;03m        tools (List of [`ChatCompletionInputTool`], *optional*):\u001b[39;00m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;124;03m            A list of tools the model may call. Currently, only functions are supported as a tool. Use this to\u001b[39;00m\n\u001b[1;32m    608\u001b[0m \u001b[38;5;124;03m            provide a list of functions the model may generate JSON inputs for.\u001b[39;00m\n\u001b[1;32m    609\u001b[0m \u001b[38;5;124;03m        extra_body (`Dict`, *optional*):\u001b[39;00m\n\u001b[1;32m    610\u001b[0m \u001b[38;5;124;03m            Additional provider-specific parameters to pass to the model. Refer to the provider's documentation\u001b[39;00m\n\u001b[1;32m    611\u001b[0m \u001b[38;5;124;03m            for supported parameters.\u001b[39;00m\n\u001b[1;32m    612\u001b[0m \u001b[38;5;124;03m    Returns:\u001b[39;00m\n\u001b[1;32m    613\u001b[0m \u001b[38;5;124;03m        [`ChatCompletionOutput`] or Iterable of [`ChatCompletionStreamOutput`]:\u001b[39;00m\n\u001b[1;32m    614\u001b[0m \u001b[38;5;124;03m        Generated text returned from the server:\u001b[39;00m\n\u001b[1;32m    615\u001b[0m \u001b[38;5;124;03m        - if `stream=False`, the generated text is returned as a [`ChatCompletionOutput`] (default).\u001b[39;00m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;124;03m        - if `stream=True`, the generated text is returned token by token as a sequence of [`ChatCompletionStreamOutput`].\u001b[39;00m\n\u001b[1;32m    617\u001b[0m \n\u001b[1;32m    618\u001b[0m \u001b[38;5;124;03m    Raises:\u001b[39;00m\n\u001b[1;32m    619\u001b[0m \u001b[38;5;124;03m        [`InferenceTimeoutError`]:\u001b[39;00m\n\u001b[1;32m    620\u001b[0m \u001b[38;5;124;03m            If the model is unavailable or the request times out.\u001b[39;00m\n\u001b[1;32m    621\u001b[0m \u001b[38;5;124;03m        `HTTPError`:\u001b[39;00m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;124;03m            If the request fails with an HTTP error status code other than HTTP 503.\u001b[39;00m\n\u001b[1;32m    623\u001b[0m \n\u001b[1;32m    624\u001b[0m \u001b[38;5;124;03m    Example:\u001b[39;00m\n\u001b[1;32m    625\u001b[0m \n\u001b[1;32m    626\u001b[0m \u001b[38;5;124;03m    ```py\u001b[39;00m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;124;03m    >>> from huggingface_hub import InferenceClient\u001b[39;00m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;124;03m    >>> messages = [{\"role\": \"user\", \"content\": \"What is the capital of France?\"}]\u001b[39;00m\n\u001b[1;32m    629\u001b[0m \u001b[38;5;124;03m    >>> client = InferenceClient(\"meta-llama/Meta-Llama-3-8B-Instruct\")\u001b[39;00m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;124;03m    >>> client.chat_completion(messages, max_tokens=100)\u001b[39;00m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;124;03m    ChatCompletionOutput(\u001b[39;00m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;124;03m        choices=[\u001b[39;00m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;124;03m            ChatCompletionOutputComplete(\u001b[39;00m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;124;03m                finish_reason='eos_token',\u001b[39;00m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;124;03m                index=0,\u001b[39;00m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;124;03m                message=ChatCompletionOutputMessage(\u001b[39;00m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;124;03m                    role='assistant',\u001b[39;00m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;124;03m                    content='The capital of France is Paris.',\u001b[39;00m\n\u001b[1;32m    639\u001b[0m \u001b[38;5;124;03m                    name=None,\u001b[39;00m\n\u001b[1;32m    640\u001b[0m \u001b[38;5;124;03m                    tool_calls=None\u001b[39;00m\n\u001b[1;32m    641\u001b[0m \u001b[38;5;124;03m                ),\u001b[39;00m\n\u001b[1;32m    642\u001b[0m \u001b[38;5;124;03m                logprobs=None\u001b[39;00m\n\u001b[1;32m    643\u001b[0m \u001b[38;5;124;03m            )\u001b[39;00m\n\u001b[1;32m    644\u001b[0m \u001b[38;5;124;03m        ],\u001b[39;00m\n\u001b[1;32m    645\u001b[0m \u001b[38;5;124;03m        created=1719907176,\u001b[39;00m\n\u001b[1;32m    646\u001b[0m \u001b[38;5;124;03m        id='',\u001b[39;00m\n\u001b[1;32m    647\u001b[0m \u001b[38;5;124;03m        model='meta-llama/Meta-Llama-3-8B-Instruct',\u001b[39;00m\n\u001b[1;32m    648\u001b[0m \u001b[38;5;124;03m        object='text_completion',\u001b[39;00m\n\u001b[1;32m    649\u001b[0m \u001b[38;5;124;03m        system_fingerprint='2.0.4-sha-f426a33',\u001b[39;00m\n\u001b[1;32m    650\u001b[0m \u001b[38;5;124;03m        usage=ChatCompletionOutputUsage(\u001b[39;00m\n\u001b[1;32m    651\u001b[0m \u001b[38;5;124;03m            completion_tokens=8,\u001b[39;00m\n\u001b[1;32m    652\u001b[0m \u001b[38;5;124;03m            prompt_tokens=17,\u001b[39;00m\n\u001b[1;32m    653\u001b[0m \u001b[38;5;124;03m            total_tokens=25\u001b[39;00m\n\u001b[1;32m    654\u001b[0m \u001b[38;5;124;03m        )\u001b[39;00m\n\u001b[1;32m    655\u001b[0m \u001b[38;5;124;03m    )\u001b[39;00m\n\u001b[1;32m    656\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[1;32m    657\u001b[0m \n\u001b[1;32m    658\u001b[0m \u001b[38;5;124;03m    Example using streaming:\u001b[39;00m\n\u001b[1;32m    659\u001b[0m \u001b[38;5;124;03m    ```py\u001b[39;00m\n\u001b[1;32m    660\u001b[0m \u001b[38;5;124;03m    >>> from huggingface_hub import InferenceClient\u001b[39;00m\n\u001b[1;32m    661\u001b[0m \u001b[38;5;124;03m    >>> messages = [{\"role\": \"user\", \"content\": \"What is the capital of France?\"}]\u001b[39;00m\n\u001b[1;32m    662\u001b[0m \u001b[38;5;124;03m    >>> client = InferenceClient(\"meta-llama/Meta-Llama-3-8B-Instruct\")\u001b[39;00m\n\u001b[1;32m    663\u001b[0m \u001b[38;5;124;03m    >>> for token in client.chat_completion(messages, max_tokens=10, stream=True):\u001b[39;00m\n\u001b[1;32m    664\u001b[0m \u001b[38;5;124;03m    ...     print(token)\u001b[39;00m\n\u001b[1;32m    665\u001b[0m \u001b[38;5;124;03m    ChatCompletionStreamOutput(choices=[ChatCompletionStreamOutputChoice(delta=ChatCompletionStreamOutputDelta(content='The', role='assistant'), index=0, finish_reason=None)], created=1710498504)\u001b[39;00m\n\u001b[1;32m    666\u001b[0m \u001b[38;5;124;03m    ChatCompletionStreamOutput(choices=[ChatCompletionStreamOutputChoice(delta=ChatCompletionStreamOutputDelta(content=' capital', role='assistant'), index=0, finish_reason=None)], created=1710498504)\u001b[39;00m\n\u001b[1;32m    667\u001b[0m \u001b[38;5;124;03m    (...)\u001b[39;00m\n\u001b[1;32m    668\u001b[0m \u001b[38;5;124;03m    ChatCompletionStreamOutput(choices=[ChatCompletionStreamOutputChoice(delta=ChatCompletionStreamOutputDelta(content=' may', role='assistant'), index=0, finish_reason=None)], created=1710498504)\u001b[39;00m\n\u001b[1;32m    669\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[1;32m    670\u001b[0m \n\u001b[1;32m    671\u001b[0m \u001b[38;5;124;03m    Example using OpenAI's syntax:\u001b[39;00m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;124;03m    ```py\u001b[39;00m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;124;03m    # instead of `from openai import OpenAI`\u001b[39;00m\n\u001b[1;32m    674\u001b[0m \u001b[38;5;124;03m    from huggingface_hub import InferenceClient\u001b[39;00m\n\u001b[1;32m    675\u001b[0m \n\u001b[1;32m    676\u001b[0m \u001b[38;5;124;03m    # instead of `client = OpenAI(...)`\u001b[39;00m\n\u001b[1;32m    677\u001b[0m \u001b[38;5;124;03m    client = InferenceClient(\u001b[39;00m\n\u001b[1;32m    678\u001b[0m \u001b[38;5;124;03m        base_url=...,\u001b[39;00m\n\u001b[1;32m    679\u001b[0m \u001b[38;5;124;03m        api_key=...,\u001b[39;00m\n\u001b[1;32m    680\u001b[0m \u001b[38;5;124;03m    )\u001b[39;00m\n\u001b[1;32m    681\u001b[0m \n\u001b[1;32m    682\u001b[0m \u001b[38;5;124;03m    output = client.chat.completions.create(\u001b[39;00m\n\u001b[1;32m    683\u001b[0m \u001b[38;5;124;03m        model=\"meta-llama/Meta-Llama-3-8B-Instruct\",\u001b[39;00m\n\u001b[1;32m    684\u001b[0m \u001b[38;5;124;03m        messages=[\u001b[39;00m\n\u001b[1;32m    685\u001b[0m \u001b[38;5;124;03m            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\u001b[39;00m\n\u001b[1;32m    686\u001b[0m \u001b[38;5;124;03m            {\"role\": \"user\", \"content\": \"Count to 10\"},\u001b[39;00m\n\u001b[1;32m    687\u001b[0m \u001b[38;5;124;03m        ],\u001b[39;00m\n\u001b[1;32m    688\u001b[0m \u001b[38;5;124;03m        stream=True,\u001b[39;00m\n\u001b[1;32m    689\u001b[0m \u001b[38;5;124;03m        max_tokens=1024,\u001b[39;00m\n\u001b[1;32m    690\u001b[0m \u001b[38;5;124;03m    )\u001b[39;00m\n\u001b[1;32m    691\u001b[0m \n\u001b[1;32m    692\u001b[0m \u001b[38;5;124;03m    for chunk in output:\u001b[39;00m\n\u001b[1;32m    693\u001b[0m \u001b[38;5;124;03m        print(chunk.choices[0].delta.content)\u001b[39;00m\n\u001b[1;32m    694\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[1;32m    695\u001b[0m \n\u001b[1;32m    696\u001b[0m \u001b[38;5;124;03m    Example using a third-party provider directly with extra (provider-specific) parameters. Usage will be billed on your Together AI account.\u001b[39;00m\n\u001b[1;32m    697\u001b[0m \u001b[38;5;124;03m    ```py\u001b[39;00m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;124;03m    >>> from huggingface_hub import InferenceClient\u001b[39;00m\n\u001b[1;32m    699\u001b[0m \u001b[38;5;124;03m    >>> client = InferenceClient(\u001b[39;00m\n\u001b[1;32m    700\u001b[0m \u001b[38;5;124;03m    ...     provider=\"together\",  # Use Together AI provider\u001b[39;00m\n\u001b[1;32m    701\u001b[0m \u001b[38;5;124;03m    ...     api_key=\"<together_api_key>\",  # Pass your Together API key directly\u001b[39;00m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;124;03m    ... )\u001b[39;00m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;124;03m    >>> client.chat_completion(\u001b[39;00m\n\u001b[1;32m    704\u001b[0m \u001b[38;5;124;03m    ...     model=\"meta-llama/Meta-Llama-3-8B-Instruct\",\u001b[39;00m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;124;03m    ...     messages=[{\"role\": \"user\", \"content\": \"What is the capital of France?\"}],\u001b[39;00m\n\u001b[0;32m--> 706\u001b[0m \u001b[38;5;124;03m    ...     extra_body={\"safety_model\": \"Meta-Llama/Llama-Guard-7b\"},\u001b[39;00m\n\u001b[1;32m    707\u001b[0m \u001b[38;5;124;03m    ... )\u001b[39;00m\n\u001b[1;32m    708\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[1;32m    709\u001b[0m \n\u001b[1;32m    710\u001b[0m \u001b[38;5;124;03m    Example using a third-party provider through Hugging Face Routing. Usage will be billed on your Hugging Face account.\u001b[39;00m\n\u001b[1;32m    711\u001b[0m \u001b[38;5;124;03m    ```py\u001b[39;00m\n\u001b[1;32m    712\u001b[0m \u001b[38;5;124;03m    >>> from huggingface_hub import InferenceClient\u001b[39;00m\n\u001b[1;32m    713\u001b[0m \u001b[38;5;124;03m    >>> client = InferenceClient(\u001b[39;00m\n\u001b[1;32m    714\u001b[0m \u001b[38;5;124;03m    ...     provider=\"sambanova\",  # Use Sambanova provider\u001b[39;00m\n\u001b[1;32m    715\u001b[0m \u001b[38;5;124;03m    ...     api_key=\"hf_...\",  # Pass your HF token\u001b[39;00m\n\u001b[1;32m    716\u001b[0m \u001b[38;5;124;03m    ... )\u001b[39;00m\n\u001b[1;32m    717\u001b[0m \u001b[38;5;124;03m    >>> client.chat_completion(\u001b[39;00m\n\u001b[1;32m    718\u001b[0m \u001b[38;5;124;03m    ...     model=\"meta-llama/Meta-Llama-3-8B-Instruct\",\u001b[39;00m\n\u001b[1;32m    719\u001b[0m \u001b[38;5;124;03m    ...     messages=[{\"role\": \"user\", \"content\": \"What is the capital of France?\"}],\u001b[39;00m\n\u001b[1;32m    720\u001b[0m \u001b[38;5;124;03m    ... )\u001b[39;00m\n\u001b[1;32m    721\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[1;32m    722\u001b[0m \n\u001b[1;32m    723\u001b[0m \u001b[38;5;124;03m    Example using Image + Text as input:\u001b[39;00m\n\u001b[1;32m    724\u001b[0m \u001b[38;5;124;03m    ```py\u001b[39;00m\n\u001b[1;32m    725\u001b[0m \u001b[38;5;124;03m    >>> from huggingface_hub import InferenceClient\u001b[39;00m\n\u001b[1;32m    726\u001b[0m \n\u001b[1;32m    727\u001b[0m \u001b[38;5;124;03m    # provide a remote URL\u001b[39;00m\n\u001b[1;32m    728\u001b[0m \u001b[38;5;124;03m    >>> image_url =\"https://cdn.britannica.com/61/93061-050-99147DCE/Statue-of-Liberty-Island-New-York-Bay.jpg\"\u001b[39;00m\n\u001b[1;32m    729\u001b[0m \u001b[38;5;124;03m    # or a base64-encoded image\u001b[39;00m\n\u001b[1;32m    730\u001b[0m \u001b[38;5;124;03m    >>> image_path = \"/path/to/image.jpeg\"\u001b[39;00m\n\u001b[1;32m    731\u001b[0m \u001b[38;5;124;03m    >>> with open(image_path, \"rb\") as f:\u001b[39;00m\n\u001b[1;32m    732\u001b[0m \u001b[38;5;124;03m    ...     base64_image = base64.b64encode(f.read()).decode(\"utf-8\")\u001b[39;00m\n\u001b[1;32m    733\u001b[0m \u001b[38;5;124;03m    >>> image_url = f\"data:image/jpeg;base64,{base64_image}\"\u001b[39;00m\n\u001b[1;32m    734\u001b[0m \n\u001b[1;32m    735\u001b[0m \u001b[38;5;124;03m    >>> client = InferenceClient(\"meta-llama/Llama-3.2-11B-Vision-Instruct\")\u001b[39;00m\n\u001b[1;32m    736\u001b[0m \u001b[38;5;124;03m    >>> output = client.chat.completions.create(\u001b[39;00m\n\u001b[1;32m    737\u001b[0m \u001b[38;5;124;03m    ...     messages=[\u001b[39;00m\n\u001b[1;32m    738\u001b[0m \u001b[38;5;124;03m    ...         {\u001b[39;00m\n\u001b[1;32m    739\u001b[0m \u001b[38;5;124;03m    ...             \"role\": \"user\",\u001b[39;00m\n\u001b[1;32m    740\u001b[0m \u001b[38;5;124;03m    ...             \"content\": [\u001b[39;00m\n\u001b[1;32m    741\u001b[0m \u001b[38;5;124;03m    ...                 {\u001b[39;00m\n\u001b[1;32m    742\u001b[0m \u001b[38;5;124;03m    ...                     \"type\": \"image_url\",\u001b[39;00m\n\u001b[1;32m    743\u001b[0m \u001b[38;5;124;03m    ...                     \"image_url\": {\"url\": image_url},\u001b[39;00m\n\u001b[1;32m    744\u001b[0m \u001b[38;5;124;03m    ...                 },\u001b[39;00m\n\u001b[1;32m    745\u001b[0m \u001b[38;5;124;03m    ...                 {\u001b[39;00m\n\u001b[1;32m    746\u001b[0m \u001b[38;5;124;03m    ...                     \"type\": \"text\",\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;124;03m    ...                     \"text\": \"Describe this image in one sentence.\",\u001b[39;00m\n\u001b[1;32m    748\u001b[0m \u001b[38;5;124;03m    ...                 },\u001b[39;00m\n\u001b[1;32m    749\u001b[0m \u001b[38;5;124;03m    ...             ],\u001b[39;00m\n\u001b[1;32m    750\u001b[0m \u001b[38;5;124;03m    ...         },\u001b[39;00m\n\u001b[1;32m    751\u001b[0m \u001b[38;5;124;03m    ...     ],\u001b[39;00m\n\u001b[1;32m    752\u001b[0m \u001b[38;5;124;03m    ... )\u001b[39;00m\n\u001b[1;32m    753\u001b[0m \u001b[38;5;124;03m    >>> output\u001b[39;00m\n\u001b[1;32m    754\u001b[0m \u001b[38;5;124;03m    The image depicts the iconic Statue of Liberty situated in New York Harbor, New York, on a clear day.\u001b[39;00m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[1;32m    756\u001b[0m \n\u001b[1;32m    757\u001b[0m \u001b[38;5;124;03m    Example using tools:\u001b[39;00m\n\u001b[1;32m    758\u001b[0m \u001b[38;5;124;03m    ```py\u001b[39;00m\n\u001b[1;32m    759\u001b[0m \u001b[38;5;124;03m    >>> client = InferenceClient(\"meta-llama/Meta-Llama-3-70B-Instruct\")\u001b[39;00m\n\u001b[1;32m    760\u001b[0m \u001b[38;5;124;03m    >>> messages = [\u001b[39;00m\n\u001b[1;32m    761\u001b[0m \u001b[38;5;124;03m    ...     {\u001b[39;00m\n\u001b[1;32m    762\u001b[0m \u001b[38;5;124;03m    ...         \"role\": \"system\",\u001b[39;00m\n\u001b[1;32m    763\u001b[0m \u001b[38;5;124;03m    ...         \"content\": \"Don't make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous.\",\u001b[39;00m\n\u001b[1;32m    764\u001b[0m \u001b[38;5;124;03m    ...     },\u001b[39;00m\n\u001b[1;32m    765\u001b[0m \u001b[38;5;124;03m    ...     {\u001b[39;00m\n\u001b[1;32m    766\u001b[0m \u001b[38;5;124;03m    ...         \"role\": \"user\",\u001b[39;00m\n\u001b[1;32m    767\u001b[0m \u001b[38;5;124;03m    ...         \"content\": \"What's the weather like the next 3 days in San Francisco, CA?\",\u001b[39;00m\n\u001b[1;32m    768\u001b[0m \u001b[38;5;124;03m    ...     },\u001b[39;00m\n\u001b[1;32m    769\u001b[0m \u001b[38;5;124;03m    ... ]\u001b[39;00m\n\u001b[1;32m    770\u001b[0m \u001b[38;5;124;03m    >>> tools = [\u001b[39;00m\n\u001b[1;32m    771\u001b[0m \u001b[38;5;124;03m    ...     {\u001b[39;00m\n\u001b[1;32m    772\u001b[0m \u001b[38;5;124;03m    ...         \"type\": \"function\",\u001b[39;00m\n\u001b[1;32m    773\u001b[0m \u001b[38;5;124;03m    ...         \"function\": {\u001b[39;00m\n\u001b[1;32m    774\u001b[0m \u001b[38;5;124;03m    ...             \"name\": \"get_current_weather\",\u001b[39;00m\n\u001b[1;32m    775\u001b[0m \u001b[38;5;124;03m    ...             \"description\": \"Get the current weather\",\u001b[39;00m\n\u001b[1;32m    776\u001b[0m \u001b[38;5;124;03m    ...             \"parameters\": {\u001b[39;00m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;124;03m    ...                 \"type\": \"object\",\u001b[39;00m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;124;03m    ...                 \"properties\": {\u001b[39;00m\n\u001b[1;32m    779\u001b[0m \u001b[38;5;124;03m    ...                     \"location\": {\u001b[39;00m\n\u001b[1;32m    780\u001b[0m \u001b[38;5;124;03m    ...                         \"type\": \"string\",\u001b[39;00m\n\u001b[1;32m    781\u001b[0m \u001b[38;5;124;03m    ...                         \"description\": \"The city and state, e.g. San Francisco, CA\",\u001b[39;00m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;124;03m    ...                     },\u001b[39;00m\n\u001b[1;32m    783\u001b[0m \u001b[38;5;124;03m    ...                     \"format\": {\u001b[39;00m\n\u001b[1;32m    784\u001b[0m \u001b[38;5;124;03m    ...                         \"type\": \"string\",\u001b[39;00m\n\u001b[1;32m    785\u001b[0m \u001b[38;5;124;03m    ...                         \"enum\": [\"celsius\", \"fahrenheit\"],\u001b[39;00m\n\u001b[1;32m    786\u001b[0m \u001b[38;5;124;03m    ...                         \"description\": \"The temperature unit to use. Infer this from the users location.\",\u001b[39;00m\n\u001b[1;32m    787\u001b[0m \u001b[38;5;124;03m    ...                     },\u001b[39;00m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;124;03m    ...                 },\u001b[39;00m\n\u001b[1;32m    789\u001b[0m \u001b[38;5;124;03m    ...                 \"required\": [\"location\", \"format\"],\u001b[39;00m\n\u001b[1;32m    790\u001b[0m \u001b[38;5;124;03m    ...             },\u001b[39;00m\n\u001b[1;32m    791\u001b[0m \u001b[38;5;124;03m    ...         },\u001b[39;00m\n\u001b[1;32m    792\u001b[0m \u001b[38;5;124;03m    ...     },\u001b[39;00m\n\u001b[1;32m    793\u001b[0m \u001b[38;5;124;03m    ...     {\u001b[39;00m\n\u001b[1;32m    794\u001b[0m \u001b[38;5;124;03m    ...         \"type\": \"function\",\u001b[39;00m\n\u001b[1;32m    795\u001b[0m \u001b[38;5;124;03m    ...         \"function\": {\u001b[39;00m\n\u001b[1;32m    796\u001b[0m \u001b[38;5;124;03m    ...             \"name\": \"get_n_day_weather_forecast\",\u001b[39;00m\n\u001b[1;32m    797\u001b[0m \u001b[38;5;124;03m    ...             \"description\": \"Get an N-day weather forecast\",\u001b[39;00m\n\u001b[1;32m    798\u001b[0m \u001b[38;5;124;03m    ...             \"parameters\": {\u001b[39;00m\n\u001b[1;32m    799\u001b[0m \u001b[38;5;124;03m    ...                 \"type\": \"object\",\u001b[39;00m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;124;03m    ...                 \"properties\": {\u001b[39;00m\n\u001b[1;32m    801\u001b[0m \u001b[38;5;124;03m    ...                     \"location\": {\u001b[39;00m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;124;03m    ...                         \"type\": \"string\",\u001b[39;00m\n\u001b[1;32m    803\u001b[0m \u001b[38;5;124;03m    ...                         \"description\": \"The city and state, e.g. San Francisco, CA\",\u001b[39;00m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;124;03m    ...                     },\u001b[39;00m\n\u001b[1;32m    805\u001b[0m \u001b[38;5;124;03m    ...                     \"format\": {\u001b[39;00m\n\u001b[1;32m    806\u001b[0m \u001b[38;5;124;03m    ...                         \"type\": \"string\",\u001b[39;00m\n\u001b[1;32m    807\u001b[0m \u001b[38;5;124;03m    ...                         \"enum\": [\"celsius\", \"fahrenheit\"],\u001b[39;00m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;124;03m    ...                         \"description\": \"The temperature unit to use. Infer this from the users location.\",\u001b[39;00m\n\u001b[1;32m    809\u001b[0m \u001b[38;5;124;03m    ...                     },\u001b[39;00m\n\u001b[1;32m    810\u001b[0m \u001b[38;5;124;03m    ...                     \"num_days\": {\u001b[39;00m\n\u001b[1;32m    811\u001b[0m \u001b[38;5;124;03m    ...                         \"type\": \"integer\",\u001b[39;00m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;124;03m    ...                         \"description\": \"The number of days to forecast\",\u001b[39;00m\n\u001b[1;32m    813\u001b[0m \u001b[38;5;124;03m    ...                     },\u001b[39;00m\n\u001b[1;32m    814\u001b[0m \u001b[38;5;124;03m    ...                 },\u001b[39;00m\n\u001b[1;32m    815\u001b[0m \u001b[38;5;124;03m    ...                 \"required\": [\"location\", \"format\", \"num_days\"],\u001b[39;00m\n\u001b[1;32m    816\u001b[0m \u001b[38;5;124;03m    ...             },\u001b[39;00m\n\u001b[1;32m    817\u001b[0m \u001b[38;5;124;03m    ...         },\u001b[39;00m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;124;03m    ...     },\u001b[39;00m\n\u001b[1;32m    819\u001b[0m \u001b[38;5;124;03m    ... ]\u001b[39;00m\n\u001b[1;32m    820\u001b[0m \n\u001b[1;32m    821\u001b[0m \u001b[38;5;124;03m    >>> response = client.chat_completion(\u001b[39;00m\n\u001b[1;32m    822\u001b[0m \u001b[38;5;124;03m    ...     model=\"meta-llama/Meta-Llama-3-70B-Instruct\",\u001b[39;00m\n\u001b[1;32m    823\u001b[0m \u001b[38;5;124;03m    ...     messages=messages,\u001b[39;00m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;124;03m    ...     tools=tools,\u001b[39;00m\n\u001b[1;32m    825\u001b[0m \u001b[38;5;124;03m    ...     tool_choice=\"auto\",\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;124;03m    ...     max_tokens=500,\u001b[39;00m\n\u001b[1;32m    827\u001b[0m \u001b[38;5;124;03m    ... )\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;124;03m    >>> response.choices[0].message.tool_calls[0].function\u001b[39;00m\n\u001b[1;32m    829\u001b[0m \u001b[38;5;124;03m    ChatCompletionOutputFunctionDefinition(\u001b[39;00m\n\u001b[1;32m    830\u001b[0m \u001b[38;5;124;03m        arguments={\u001b[39;00m\n\u001b[1;32m    831\u001b[0m \u001b[38;5;124;03m            'location': 'San Francisco, CA',\u001b[39;00m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;124;03m            'format': 'fahrenheit',\u001b[39;00m\n\u001b[1;32m    833\u001b[0m \u001b[38;5;124;03m            'num_days': 3\u001b[39;00m\n\u001b[1;32m    834\u001b[0m \u001b[38;5;124;03m        },\u001b[39;00m\n\u001b[1;32m    835\u001b[0m \u001b[38;5;124;03m        name='get_n_day_weather_forecast',\u001b[39;00m\n\u001b[1;32m    836\u001b[0m \u001b[38;5;124;03m        description=None\u001b[39;00m\n\u001b[1;32m    837\u001b[0m \u001b[38;5;124;03m    )\u001b[39;00m\n\u001b[1;32m    838\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[1;32m    839\u001b[0m \n\u001b[1;32m    840\u001b[0m \u001b[38;5;124;03m    Example using response_format:\u001b[39;00m\n\u001b[1;32m    841\u001b[0m \u001b[38;5;124;03m    ```py\u001b[39;00m\n\u001b[1;32m    842\u001b[0m \u001b[38;5;124;03m    >>> from huggingface_hub import InferenceClient\u001b[39;00m\n\u001b[1;32m    843\u001b[0m \u001b[38;5;124;03m    >>> client = InferenceClient(\"meta-llama/Meta-Llama-3-70B-Instruct\")\u001b[39;00m\n\u001b[1;32m    844\u001b[0m \u001b[38;5;124;03m    >>> messages = [\u001b[39;00m\n\u001b[1;32m    845\u001b[0m \u001b[38;5;124;03m    ...     {\u001b[39;00m\n\u001b[1;32m    846\u001b[0m \u001b[38;5;124;03m    ...         \"role\": \"user\",\u001b[39;00m\n\u001b[1;32m    847\u001b[0m \u001b[38;5;124;03m    ...         \"content\": \"I saw a puppy a cat and a raccoon during my bike ride in the park. What did I saw and when?\",\u001b[39;00m\n\u001b[1;32m    848\u001b[0m \u001b[38;5;124;03m    ...     },\u001b[39;00m\n\u001b[1;32m    849\u001b[0m \u001b[38;5;124;03m    ... ]\u001b[39;00m\n\u001b[1;32m    850\u001b[0m \u001b[38;5;124;03m    >>> response_format = {\u001b[39;00m\n\u001b[1;32m    851\u001b[0m \u001b[38;5;124;03m    ...     \"type\": \"json\",\u001b[39;00m\n\u001b[1;32m    852\u001b[0m \u001b[38;5;124;03m    ...     \"value\": {\u001b[39;00m\n\u001b[1;32m    853\u001b[0m \u001b[38;5;124;03m    ...         \"properties\": {\u001b[39;00m\n\u001b[1;32m    854\u001b[0m \u001b[38;5;124;03m    ...             \"location\": {\"type\": \"string\"},\u001b[39;00m\n\u001b[1;32m    855\u001b[0m \u001b[38;5;124;03m    ...             \"activity\": {\"type\": \"string\"},\u001b[39;00m\n\u001b[1;32m    856\u001b[0m \u001b[38;5;124;03m    ...             \"animals_seen\": {\"type\": \"integer\", \"minimum\": 1, \"maximum\": 5},\u001b[39;00m\n\u001b[1;32m    857\u001b[0m \u001b[38;5;124;03m    ...             \"animals\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\u001b[39;00m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;124;03m    ...         },\u001b[39;00m\n\u001b[1;32m    859\u001b[0m \u001b[38;5;124;03m    ...         \"required\": [\"location\", \"activity\", \"animals_seen\", \"animals\"],\u001b[39;00m\n\u001b[1;32m    860\u001b[0m \u001b[38;5;124;03m    ...     },\u001b[39;00m\n\u001b[1;32m    861\u001b[0m \u001b[38;5;124;03m    ... }\u001b[39;00m\n\u001b[1;32m    862\u001b[0m \u001b[38;5;124;03m    >>> response = client.chat_completion(\u001b[39;00m\n\u001b[1;32m    863\u001b[0m \u001b[38;5;124;03m    ...     messages=messages,\u001b[39;00m\n\u001b[1;32m    864\u001b[0m \u001b[38;5;124;03m    ...     response_format=response_format,\u001b[39;00m\n\u001b[1;32m    865\u001b[0m \u001b[38;5;124;03m    ...     max_tokens=500,\u001b[39;00m\n\u001b[1;32m    866\u001b[0m \u001b[38;5;124;03m    ... )\u001b[39;00m\n\u001b[1;32m    867\u001b[0m \u001b[38;5;124;03m    >>> response.choices[0].message.content\u001b[39;00m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;124;03m    '{\\n\\n\"activity\": \"bike ride\",\\n\"animals\": [\"puppy\", \"cat\", \"raccoon\"],\\n\"animals_seen\": 3,\\n\"location\": \"park\"}'\u001b[39;00m\n\u001b[1;32m    869\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[1;32m    870\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;66;03m# Since `chat_completion(..., model=xxx)` is also a payload parameter for the server, we need to handle 'model' differently.\u001b[39;00m\n\u001b[1;32m    872\u001b[0m     \u001b[38;5;66;03m# `self.model` takes precedence over 'model' argument for building URL.\u001b[39;00m\n\u001b[1;32m    873\u001b[0m     \u001b[38;5;66;03m# `model` takes precedence for payload value.\u001b[39;00m\n\u001b[1;32m    874\u001b[0m     model_id_or_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;129;01mor\u001b[39;00m model\n",
      "File \u001b[0;32m~/anaconda3/envs/medibot/lib/python3.10/site-packages/huggingface_hub/inference/_client.py:273\u001b[0m, in \u001b[0;36mInferenceClient.post\u001b[0;34m(self, json, data, model, task, stream)\u001b[0m\n\u001b[1;32m    270\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InferenceTimeoutError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInference call timed out: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merror\u001b[39;00m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 273\u001b[0m     \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39miter_lines() \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m response\u001b[38;5;241m.\u001b[39mcontent\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m error:\n",
      "File \u001b[0;32m~/anaconda3/envs/medibot/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py:371\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    367\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HfHubHTTPError(message, response\u001b[38;5;241m=\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;66;03m# Convert `HTTPError` into a `HfHubHTTPError` to display request information\u001b[39;00m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;66;03m# as well (request id and/or server error message)\u001b[39;00m\n\u001b[0;32m--> 371\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m HfHubHTTPError(\u001b[38;5;28mstr\u001b[39m(e), response\u001b[38;5;241m=\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mHfHubHTTPError\u001b[0m: 410 Client Error: Gone for url: https://api-inference.huggingface.co/models/TheBloke/guanaco-7B-HF/v1/chat/completions\n\nhttps://api-inference.huggingface.co is no longer supported. Please use https://router.huggingface.co instead."
     ]
    }
   ],
   "source": [
    "# model = ChatHuggingFace(llm=chatModel)\n",
    "\n",
    "# result = model.invoke(\"What is the capital of India?\")\n",
    "\n",
    "# print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "11168871",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c89f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# system_prompt = (\n",
    "#     \"You are an Medical assistant for question-answering tasks. \"\n",
    "#     \"Use the following pieces of retrieved context to answer \"\n",
    "#     \"the question. If you don't know the answer, say that you \"\n",
    "#     \"don't know. Use three sentences maximum and keep the \"\n",
    "#     \"answer concise.\"\n",
    "#     \"\\n\\n\"\n",
    "#     \"{context}\"\n",
    "# )\n",
    "\n",
    "\n",
    "# prompt = ChatPromptTemplate.from_messages(\n",
    "#     [\n",
    "#         (\"system\", system_prompt),\n",
    "#         (\"human\", \"{input}\"),\n",
    "#     ]\n",
    "# )\n",
    "system_prompt = (\n",
    "    \"You are a medical assistant. Use the provided context to answer. \"\n",
    "    \"If the answer is not in the context, say you do not know. \"\n",
    "    \"Use at most three concise sentences.\\n\\n{context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "143b991d",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_answer_chain = create_stuff_documents_chain(chatModel, prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "551bc55d",
   "metadata": {},
   "outputs": [
    {
     "ename": "HfHubHTTPError",
     "evalue": "410 Client Error: Gone for url: https://api-inference.huggingface.co/models/meta-llama/Llama-3.1-8B-Instruct\n\nhttps://api-inference.huggingface.co is no longer supported. Please use https://router.huggingface.co instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/medibot/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py:304\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 304\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/envs/medibot/lib/python3.10/site-packages/requests/models.py:1026\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1026\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 410 Client Error: Gone for url: https://api-inference.huggingface.co/models/meta-llama/Llama-3.1-8B-Instruct",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mHfHubHTTPError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrag_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwhat is Acromegaly and gigantism?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m~/anaconda3/envs/medibot/lib/python3.10/site-packages/langchain_core/runnables/base.py:5711\u001b[0m, in \u001b[0;36mRunnableBindingBase.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   5704\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m   5705\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m   5706\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5709\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[1;32m   5710\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Output:\n\u001b[0;32m-> 5711\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbound\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5712\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5713\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_merge_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5714\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5715\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/medibot/lib/python3.10/site-packages/langchain_core/runnables/base.py:3246\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3244\u001b[0m                 input_ \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, input_, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   3245\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3246\u001b[0m                 input_ \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3247\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   3248\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/envs/medibot/lib/python3.10/site-packages/langchain_core/runnables/passthrough.py:530\u001b[0m, in \u001b[0;36mRunnableAssign.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m    524\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    525\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    528\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    529\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[0;32m--> 530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_with_config\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_invoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/medibot/lib/python3.10/site-packages/langchain_core/runnables/base.py:2092\u001b[0m, in \u001b[0;36mRunnable._call_with_config\u001b[0;34m(self, func, input_, config, run_type, serialized, **kwargs)\u001b[0m\n\u001b[1;32m   2088\u001b[0m     child_config \u001b[38;5;241m=\u001b[39m patch_config(config, callbacks\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mget_child())\n\u001b[1;32m   2089\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(child_config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[1;32m   2090\u001b[0m         output \u001b[38;5;241m=\u001b[39m cast(\n\u001b[1;32m   2091\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutput\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m-> 2092\u001b[0m             \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2093\u001b[0m \u001b[43m                \u001b[49m\u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   2094\u001b[0m \u001b[43m                \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2095\u001b[0m \u001b[43m                \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2096\u001b[0m \u001b[43m                \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2097\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2098\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2099\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m   2100\u001b[0m         )\n\u001b[1;32m   2101\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   2102\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m~/anaconda3/envs/medibot/lib/python3.10/site-packages/langchain_core/runnables/config.py:430\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[0;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    428\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[1;32m    429\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m run_manager\n\u001b[0;32m--> 430\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/medibot/lib/python3.10/site-packages/langchain_core/runnables/passthrough.py:516\u001b[0m, in \u001b[0;36mRunnableAssign._invoke\u001b[0;34m(self, value, run_manager, config, **kwargs)\u001b[0m\n\u001b[1;32m    511\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe input to RunnablePassthrough.assign() must be a dict.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)  \u001b[38;5;66;03m# noqa: TRY004\u001b[39;00m\n\u001b[1;32m    514\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m    515\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mvalue,\n\u001b[0;32m--> 516\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    517\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpatch_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    519\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    520\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    521\u001b[0m }\n",
      "File \u001b[0;32m~/anaconda3/envs/medibot/lib/python3.10/site-packages/langchain_core/runnables/base.py:4001\u001b[0m, in \u001b[0;36mRunnableParallel.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3996\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m get_executor_for_config(config) \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[1;32m   3997\u001b[0m         futures \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m   3998\u001b[0m             executor\u001b[38;5;241m.\u001b[39msubmit(_invoke_step, step, \u001b[38;5;28minput\u001b[39m, config, key)\n\u001b[1;32m   3999\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m key, step \u001b[38;5;129;01min\u001b[39;00m steps\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m   4000\u001b[0m         ]\n\u001b[0;32m-> 4001\u001b[0m         output \u001b[38;5;241m=\u001b[39m {key: future\u001b[38;5;241m.\u001b[39mresult() \u001b[38;5;28;01mfor\u001b[39;00m key, future \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(steps, futures)}\n\u001b[1;32m   4002\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   4003\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/envs/medibot/lib/python3.10/site-packages/langchain_core/runnables/base.py:4001\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   3996\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m get_executor_for_config(config) \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[1;32m   3997\u001b[0m         futures \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m   3998\u001b[0m             executor\u001b[38;5;241m.\u001b[39msubmit(_invoke_step, step, \u001b[38;5;28minput\u001b[39m, config, key)\n\u001b[1;32m   3999\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m key, step \u001b[38;5;129;01min\u001b[39;00m steps\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m   4000\u001b[0m         ]\n\u001b[0;32m-> 4001\u001b[0m         output \u001b[38;5;241m=\u001b[39m {key: \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m key, future \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(steps, futures)}\n\u001b[1;32m   4002\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   4003\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/envs/medibot/lib/python3.10/concurrent/futures/_base.py:458\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 458\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    460\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "File \u001b[0;32m~/anaconda3/envs/medibot/lib/python3.10/concurrent/futures/_base.py:403\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 403\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    404\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    405\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/medibot/lib/python3.10/concurrent/futures/thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "File \u001b[0;32m~/anaconda3/envs/medibot/lib/python3.10/site-packages/langchain_core/runnables/base.py:3985\u001b[0m, in \u001b[0;36mRunnableParallel.invoke.<locals>._invoke_step\u001b[0;34m(step, input_, config, key)\u001b[0m\n\u001b[1;32m   3979\u001b[0m child_config \u001b[38;5;241m=\u001b[39m patch_config(\n\u001b[1;32m   3980\u001b[0m     config,\n\u001b[1;32m   3981\u001b[0m     \u001b[38;5;66;03m# mark each step as a child run\u001b[39;00m\n\u001b[1;32m   3982\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mget_child(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmap:key:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   3983\u001b[0m )\n\u001b[1;32m   3984\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(child_config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[0;32m-> 3985\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3986\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3987\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3988\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchild_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3989\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/medibot/lib/python3.10/site-packages/langchain_core/runnables/base.py:5711\u001b[0m, in \u001b[0;36mRunnableBindingBase.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   5704\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m   5705\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m   5706\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5709\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[1;32m   5710\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Output:\n\u001b[0;32m-> 5711\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbound\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5712\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5713\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_merge_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5714\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5715\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/medibot/lib/python3.10/site-packages/langchain_core/runnables/base.py:3246\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3244\u001b[0m                 input_ \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, input_, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   3245\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3246\u001b[0m                 input_ \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3247\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   3248\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/envs/medibot/lib/python3.10/site-packages/langchain_core/language_models/llms.py:392\u001b[0m, in \u001b[0;36mBaseLLM.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    383\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    389\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    390\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    391\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m--> 392\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    402\u001b[0m         \u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    403\u001b[0m         \u001b[38;5;241m.\u001b[39mtext\n\u001b[1;32m    404\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/medibot/lib/python3.10/site-packages/langchain_core/language_models/llms.py:791\u001b[0m, in \u001b[0;36mBaseLLM.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m    783\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    784\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    788\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    789\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    790\u001b[0m     prompt_strings \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_string() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 791\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_strings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/medibot/lib/python3.10/site-packages/langchain_core/language_models/llms.py:1002\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[0;34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    987\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m get_llm_cache() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m    988\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    989\u001b[0m         callback_manager\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[1;32m    990\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_serialized,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1000\u001b[0m         )\n\u001b[1;32m   1001\u001b[0m     ]\n\u001b[0;32m-> 1002\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1003\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1004\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1005\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1006\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnew_arg_supported\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnew_arg_supported\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1007\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1008\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1009\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1010\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m   1011\u001b[0m         callback_managers[idx]\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[1;32m   1012\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_serialized,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m missing_prompt_idxs\n\u001b[1;32m   1020\u001b[0m     ]\n",
      "File \u001b[0;32m~/anaconda3/envs/medibot/lib/python3.10/site-packages/langchain_core/language_models/llms.py:817\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    806\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_generate_helper\u001b[39m(\n\u001b[1;32m    807\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    808\u001b[0m     prompts: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    814\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    815\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    816\u001b[0m         output \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 817\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    818\u001b[0m \u001b[43m                \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    819\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    820\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# TODO: support multiple run managers\u001b[39;49;00m\n\u001b[1;32m    821\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    822\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    823\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    824\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    825\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(prompts, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[1;32m    826\u001b[0m         )\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    828\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/anaconda3/envs/medibot/lib/python3.10/site-packages/langchain_core/language_models/llms.py:1580\u001b[0m, in \u001b[0;36mLLM._generate\u001b[0;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1577\u001b[0m new_arg_supported \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1578\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts:\n\u001b[1;32m   1579\u001b[0m     text \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m-> 1580\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1581\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m   1582\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(prompt, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1583\u001b[0m     )\n\u001b[1;32m   1584\u001b[0m     generations\u001b[38;5;241m.\u001b[39mappend([Generation(text\u001b[38;5;241m=\u001b[39mtext)])\n\u001b[1;32m   1585\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m LLMResult(generations\u001b[38;5;241m=\u001b[39mgenerations)\n",
      "File \u001b[0;32m~/anaconda3/envs/medibot/lib/python3.10/site-packages/langchain_huggingface/llms/huggingface_endpoint.py:312\u001b[0m, in \u001b[0;36mHuggingFaceEndpoint._call\u001b[0;34m(self, prompt, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    309\u001b[0m     invocation_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m invocation_params[\n\u001b[1;32m    310\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop_sequences\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    311\u001b[0m     ]  \u001b[38;5;66;03m# porting 'stop_sequences' into the 'stop' argument\u001b[39;00m\n\u001b[0;32m--> 312\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minputs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparameters\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minvocation_params\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    317\u001b[0m     response_text \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(response\u001b[38;5;241m.\u001b[39mdecode())[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;66;03m# Maybe the generation has stopped at one of the stop sequences:\u001b[39;00m\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;66;03m# then we remove this stop sequence from the end of the generated text\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/medibot/lib/python3.10/site-packages/huggingface_hub/inference/_client.py:273\u001b[0m, in \u001b[0;36mInferenceClient.post\u001b[0;34m(self, json, data, model, task, stream)\u001b[0m\n\u001b[1;32m    270\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InferenceTimeoutError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInference call timed out: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merror\u001b[39;00m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 273\u001b[0m     \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39miter_lines() \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m response\u001b[38;5;241m.\u001b[39mcontent\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m error:\n",
      "File \u001b[0;32m~/anaconda3/envs/medibot/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py:371\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    367\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HfHubHTTPError(message, response\u001b[38;5;241m=\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;66;03m# Convert `HTTPError` into a `HfHubHTTPError` to display request information\u001b[39;00m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;66;03m# as well (request id and/or server error message)\u001b[39;00m\n\u001b[0;32m--> 371\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m HfHubHTTPError(\u001b[38;5;28mstr\u001b[39m(e), response\u001b[38;5;241m=\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mHfHubHTTPError\u001b[0m: 410 Client Error: Gone for url: https://api-inference.huggingface.co/models/meta-llama/Llama-3.1-8B-Instruct\n\nhttps://api-inference.huggingface.co is no longer supported. Please use https://router.huggingface.co instead."
     ]
    }
   ],
   "source": [
    "response = rag_chain.invoke({\"input\": \"what is Acromegaly and gigantism?\"})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9696f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = rag_chain.invoke({\"input\": \"what is Acne?\"})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c1c861",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = rag_chain.invoke({\"input\": \"what is the Treatment of Acne?\"})\n",
    "print(response[\"answer\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medibot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
